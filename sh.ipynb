{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26613633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python SFT.py --work 13 --workfile ./DATA/test.json         --Lora_MODEL None\n",
    "# !python SFT.py --work 13 --workfile ./DATA/train.json        --Lora_MODEL ./13test\n",
    "# !python SFT.py --work 12 --workfile ./DATA/outputnew3CC.json --Lora_MODEL ./13train\n",
    "# !python SFT.py --work 12 --workfile ./DATA/outputnew4CC.json --Lora_MODEL ./12outputnew3CC\n",
    "!python SFT.py --work 3  --workfile ./DATA/outputnew3CC.json --Lora_MODEL ./12outputnew4CC\n",
    "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.                           \n",
    "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
    "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
    "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.51.3.\n",
    "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 22.159 GB. Platform: Linux.\n",
    "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
    "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
    " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
    "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
    "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.35it/s]\n",
    "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.2.\n",
    "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
    "Unsloth: bias = `none` is supported for fast patching. You are using bias = lora_only.\n",
    "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
    "/llm/.conda/lib/python3.10/site-packages/peft/tuners/lora/config.py:576: UserWarning: `loftq_config` specified but will be ignored when `init_lora_weights` is not 'loftq'.\n",
    "  warnings.warn(\"`loftq_config` specified but will be ignored when `init_lora_weights` is not 'loftq'.\")\n",
    "Unsloth 2025.3.19 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n",
    "Unsloth: Tokenizing [\"prompt\"] (num_proc=2): 100%|█| 372/372 [00:01<00:00, 371.5\n",
    "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
    "   \\\\   /|    Num examples = 372 | Num Epochs = 1 | Total steps = 23\n",
    "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
    "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
    " \"-____-\"     Trainable parameters = 322,961,408/7,000,000,000 (4.61% trained)\n",
    "swanlab: Tracking run with swanlab version 0.5.5                                   \n",
    "swanlab: Run data will be saved locally in /llm/swanlog/run-20250420_154016-0e8cd89d\n",
    "swanlab: 👋 Hi queziaa, welcome to swanlab!\n",
    "swanlab: Syncing run ./lora to the cloud\n",
    "swanlab: 🏠 View project at https://swanlab.cn/@queziaa/llm\n",
    "swanlab: 🚀 View run at https://swanlab.cn/@queziaa/llm/runs/4rcpfs8pmiocy1y4yzrjo\n",
    "  0%|                                                    | 0/23 [00:00<?, ?it/s]Unsloth: Will smartly offload gradients to save VRAM!\n",
    "{'loss': 7.8171, 'grad_norm': 71.86101531982422, 'learning_rate': 0.0, 'epoch': 0.04}\n",
    "{'loss': 7.5622, 'grad_norm': 60.409576416015625, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.09}\n",
    "{'loss': 7.0107, 'grad_norm': 55.384918212890625, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.13}\n",
    "{'loss': 6.1247, 'grad_norm': 47.21338653564453, 'learning_rate': 1.2e-05, 'epoch': 0.17}\n",
    "{'loss': 4.8881, 'grad_norm': 23.30931854248047, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.22}\n",
    "{'loss': 3.9367, 'grad_norm': 22.699432373046875, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
    "{'loss': 3.3719, 'grad_norm': 17.40357208251953, 'learning_rate': 1.888888888888889e-05, 'epoch': 0.3}\n",
    "{'loss': 3.6271, 'grad_norm': 13.528074264526367, 'learning_rate': 1.7777777777777777e-05, 'epoch': 0.34}\n",
    "{'loss': 3.383, 'grad_norm': 12.708900451660156, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.39}\n",
    "{'loss': 3.5381, 'grad_norm': 15.294955253601074, 'learning_rate': 1.555555555555556e-05, 'epoch': 0.43}\n",
    "{'loss': 3.0676, 'grad_norm': 10.437663078308105, 'learning_rate': 1.4444444444444446e-05, 'epoch': 0.47}\n",
    "{'loss': 2.954, 'grad_norm': 11.774557113647461, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.52}\n",
    "{'loss': 3.1101, 'grad_norm': 12.095623016357422, 'learning_rate': 1.2222222222222224e-05, 'epoch': 0.56}\n",
    "{'loss': 3.2814, 'grad_norm': 10.446162223815918, 'learning_rate': 1.1111111111111113e-05, 'epoch': 0.6}\n",
    "{'loss': 3.1051, 'grad_norm': 11.088081359863281, 'learning_rate': 1e-05, 'epoch': 0.65}\n",
    "{'loss': 2.9316, 'grad_norm': 9.794706344604492, 'learning_rate': 8.888888888888888e-06, 'epoch': 0.69}\n",
    "{'loss': 2.5676, 'grad_norm': 9.593697547912598, 'learning_rate': 7.77777777777778e-06, 'epoch': 0.73}\n",
    "{'loss': 2.9224, 'grad_norm': 11.178924560546875, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.77}\n",
    "{'loss': 2.8085, 'grad_norm': 10.05243968963623, 'learning_rate': 5.555555555555557e-06, 'epoch': 0.82}\n",
    "{'loss': 2.7415, 'grad_norm': 10.7794189453125, 'learning_rate': 4.444444444444444e-06, 'epoch': 0.86}\n",
    "{'loss': 3.0557, 'grad_norm': 10.323504447937012, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.9}\n",
    "{'loss': 2.71, 'grad_norm': 8.980114936828613, 'learning_rate': 2.222222222222222e-06, 'epoch': 0.95}\n",
    "{'loss': 2.4385, 'grad_norm': 9.556655883789062, 'learning_rate': 1.111111111111111e-06, 'epoch': 0.99}\n",
    "100%|███████████████████████████████████████████| 23/23 [00:44<00:00,  1.78s/it]swanlab: Step 23 on key train/epoch already exists, ignored.\n",
    "swanlab: Step 23 on key train/global_step already exists, ignored.\n",
    "{'train_runtime': 50.52, 'train_samples_per_second': 7.363, 'train_steps_per_second': 0.455, 'train_loss': 3.8675532755644424, 'epoch': 0.99}\n",
    "100%|███████████████████████████████████████████| 23/23 [00:48<00:00,  2.13s/it]\n",
    "Renamed ./lora/checkpoint-23 to ./13test\n",
    "swanlab: Experiment ./lora has completed\n",
    "swanlab: 🏠 View project at https://swanlab.cn/@queziaa/llm\n",
    "swanlab: 🚀 View run at https://swanlab.cn/@queziaa/llm/runs/4rcpfs8pmiocy1y4yzrjo\n",
    "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.                           \n",
    "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
    "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
    "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.51.3.\n",
    "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 22.159 GB. Platform: Linux.\n",
    "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
    "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
    " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
    "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
    "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.36it/s]\n",
    "Unsloth 2025.3.19 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n",
    "Unsloth: Tokenizing [\"prompt\"] (num_proc=2): 100%|█| 772/772 [00:01<00:00, 759.2\n",
    "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
    "   \\\\   /|    Num examples = 772 | Num Epochs = 1 | Total steps = 48\n",
    "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
    "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
    " \"-____-\"     Trainable parameters = 322,961,408/7,000,000,000 (4.61% trained)\n",
    "swanlab: Tracking run with swanlab version 0.5.5                                   \n",
    "swanlab: Run data will be saved locally in /llm/swanlog/run-20250420_154335-0e8cd89d\n",
    "swanlab: 👋 Hi queziaa, welcome to swanlab!\n",
    "swanlab: Syncing run ./lora to the cloud\n",
    "swanlab: 🏠 View project at https://swanlab.cn/@queziaa/llm\n",
    "swanlab: 🚀 View run at https://swanlab.cn/@queziaa/llm/runs/13mrxa7zktij27gbyiay2\n",
    "  0%|                                                    | 0/48 [00:00<?, ?it/s]Unsloth: Will smartly offload gradients to save VRAM!\n",
    "{'loss': 2.6919, 'grad_norm': 9.520952224731445, 'learning_rate': 0.0, 'epoch': 0.02}\n",
    "{'loss': 2.9183, 'grad_norm': 9.894940376281738, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.04}\n",
    "{'loss': 3.327, 'grad_norm': 8.946008682250977, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.06}\n",
    "{'loss': 2.5596, 'grad_norm': 8.865336418151855, 'learning_rate': 1.2e-05, 'epoch': 0.08}\n",
    "{'loss': 3.13, 'grad_norm': 8.709087371826172, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.1}\n",
    "{'loss': 2.9542, 'grad_norm': 9.467528343200684, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
    "{'loss': 2.7412, 'grad_norm': 11.89463996887207, 'learning_rate': 1.9534883720930235e-05, 'epoch': 0.15}\n",
    "{'loss': 2.6861, 'grad_norm': 8.25811767578125, 'learning_rate': 1.9069767441860468e-05, 'epoch': 0.17}\n",
    "{'loss': 2.41, 'grad_norm': 9.435579299926758, 'learning_rate': 1.86046511627907e-05, 'epoch': 0.19}\n",
    "{'loss': 2.6044, 'grad_norm': 7.623140335083008, 'learning_rate': 1.813953488372093e-05, 'epoch': 0.21}\n",
    "{'loss': 2.8657, 'grad_norm': 8.06152629852295, 'learning_rate': 1.7674418604651163e-05, 'epoch': 0.23}\n",
    "{'loss': 2.4829, 'grad_norm': 8.856474876403809, 'learning_rate': 1.7209302325581396e-05, 'epoch': 0.25}\n",
    "{'loss': 2.5149, 'grad_norm': 9.037973403930664, 'learning_rate': 1.674418604651163e-05, 'epoch': 0.27}\n",
    "{'loss': 2.7151, 'grad_norm': 7.761199474334717, 'learning_rate': 1.6279069767441862e-05, 'epoch': 0.29}\n",
    "{'loss': 2.3448, 'grad_norm': 7.931274890899658, 'learning_rate': 1.5813953488372095e-05, 'epoch': 0.31}\n",
    "{'loss': 2.2153, 'grad_norm': 7.799591064453125, 'learning_rate': 1.5348837209302328e-05, 'epoch': 0.33}\n",
    "{'loss': 2.5048, 'grad_norm': 8.635932922363281, 'learning_rate': 1.488372093023256e-05, 'epoch': 0.35}\n",
    "{'loss': 2.2387, 'grad_norm': 7.548990249633789, 'learning_rate': 1.441860465116279e-05, 'epoch': 0.37}\n",
    "{'loss': 2.6994, 'grad_norm': 9.773680686950684, 'learning_rate': 1.3953488372093025e-05, 'epoch': 0.39}\n",
    "{'loss': 2.9633, 'grad_norm': 8.712136268615723, 'learning_rate': 1.3488372093023257e-05, 'epoch': 0.41}\n",
    "{'loss': 2.1252, 'grad_norm': 8.128839492797852, 'learning_rate': 1.302325581395349e-05, 'epoch': 0.44}\n",
    "{'loss': 2.3077, 'grad_norm': 7.623821258544922, 'learning_rate': 1.2558139534883723e-05, 'epoch': 0.46}\n",
    "{'loss': 2.6764, 'grad_norm': 7.509154319763184, 'learning_rate': 1.2093023255813954e-05, 'epoch': 0.48}\n",
    "{'loss': 2.9824, 'grad_norm': 9.656314849853516, 'learning_rate': 1.1627906976744187e-05, 'epoch': 0.5}\n",
    "{'loss': 2.7128, 'grad_norm': 7.698515892028809, 'learning_rate': 1.116279069767442e-05, 'epoch': 0.52}\n",
    "{'loss': 2.3691, 'grad_norm': 7.840996742248535, 'learning_rate': 1.0697674418604651e-05, 'epoch': 0.54}\n",
    "{'loss': 2.8612, 'grad_norm': 8.587064743041992, 'learning_rate': 1.0232558139534884e-05, 'epoch': 0.56}\n",
    "{'loss': 2.3763, 'grad_norm': 8.725162506103516, 'learning_rate': 9.767441860465117e-06, 'epoch': 0.58}\n",
    "{'loss': 2.675, 'grad_norm': 7.963301181793213, 'learning_rate': 9.30232558139535e-06, 'epoch': 0.6}\n",
    "{'loss': 2.2286, 'grad_norm': 6.793860912322998, 'learning_rate': 8.837209302325582e-06, 'epoch': 0.62}\n",
    "{'loss': 2.844, 'grad_norm': 8.376094818115234, 'learning_rate': 8.372093023255815e-06, 'epoch': 0.64}\n",
    "{'loss': 2.7737, 'grad_norm': 7.403148174285889, 'learning_rate': 7.906976744186048e-06, 'epoch': 0.66}\n",
    "{'loss': 2.8299, 'grad_norm': 7.721986293792725, 'learning_rate': 7.44186046511628e-06, 'epoch': 0.68}\n",
    "{'loss': 2.7038, 'grad_norm': 7.589362621307373, 'learning_rate': 6.976744186046513e-06, 'epoch': 0.7}\n",
    "{'loss': 2.2674, 'grad_norm': 7.6150689125061035, 'learning_rate': 6.511627906976745e-06, 'epoch': 0.73}\n",
    "{'loss': 2.3311, 'grad_norm': 6.926319122314453, 'learning_rate': 6.046511627906977e-06, 'epoch': 0.75}\n",
    "{'loss': 2.5029, 'grad_norm': 8.022195816040039, 'learning_rate': 5.58139534883721e-06, 'epoch': 0.77}\n",
    "{'loss': 2.7003, 'grad_norm': 7.315184593200684, 'learning_rate': 5.116279069767442e-06, 'epoch': 0.79}\n",
    "{'loss': 2.4453, 'grad_norm': 8.51996898651123, 'learning_rate': 4.651162790697675e-06, 'epoch': 0.81}\n",
    "{'loss': 2.3752, 'grad_norm': 7.037156581878662, 'learning_rate': 4.186046511627907e-06, 'epoch': 0.83}\n",
    "{'loss': 2.5805, 'grad_norm': 7.818453788757324, 'learning_rate': 3.72093023255814e-06, 'epoch': 0.85}\n",
    "{'loss': 2.4924, 'grad_norm': 7.494061470031738, 'learning_rate': 3.2558139534883724e-06, 'epoch': 0.87}\n",
    "{'loss': 2.7324, 'grad_norm': 8.854507446289062, 'learning_rate': 2.790697674418605e-06, 'epoch': 0.89}\n",
    "{'loss': 2.3628, 'grad_norm': 7.0314812660217285, 'learning_rate': 2.3255813953488376e-06, 'epoch': 0.91}\n",
    "{'loss': 2.9766, 'grad_norm': 7.947132110595703, 'learning_rate': 1.86046511627907e-06, 'epoch': 0.93}\n",
    "{'loss': 2.5489, 'grad_norm': 7.619377136230469, 'learning_rate': 1.3953488372093025e-06, 'epoch': 0.95}\n",
    "{'loss': 2.7139, 'grad_norm': 7.836549282073975, 'learning_rate': 9.30232558139535e-07, 'epoch': 0.97}\n",
    "{'loss': 2.909, 'grad_norm': 7.459726333618164, 'learning_rate': 4.651162790697675e-07, 'epoch': 0.99}\n",
    "100%|███████████████████████████████████████████| 48/48 [01:23<00:00,  1.72s/it]swanlab: Step 48 on key train/epoch already exists, ignored.\n",
    "swanlab: Step 48 on key train/global_step already exists, ignored.\n",
    "{'train_runtime': 89.9728, 'train_samples_per_second': 8.58, 'train_steps_per_second': 0.533, 'train_loss': 2.624423315127691, 'epoch': 0.99}\n",
    "100%|███████████████████████████████████████████| 48/48 [01:28<00:00,  1.84s/it]\n",
    "Renamed ./lora/checkpoint-48 to ./13train\n",
    "swanlab: Experiment ./lora has completed\n",
    "swanlab: 🏠 View project at https://swanlab.cn/@queziaa/llm\n",
    "swanlab: 🚀 View run at https://swanlab.cn/@queziaa/llm/runs/13mrxa7zktij27gbyiay2\n",
    "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.                           \n",
    "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
    "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
    "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.51.3.\n",
    "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 22.159 GB. Platform: Linux.\n",
    "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
    "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
    " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
    "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
    "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.38it/s]\n",
    "Unsloth 2025.3.19 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n",
    "Unsloth: Tokenizing [\"prompt\"] (num_proc=2): 100%|█| 3517/3517 [00:01<00:00, 197\n",
    "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
    "   \\\\   /|    Num examples = 3,517 | Num Epochs = 1 | Total steps = 220\n",
    "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
    "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
    " \"-____-\"     Trainable parameters = 322,961,408/7,000,000,000 (4.61% trained)\n",
    "swanlab: Tracking run with swanlab version 0.5.5                                   \n",
    "swanlab: Run data will be saved locally in /llm/swanlog/run-20250420_154734-0e8cd89d\n",
    "swanlab: 👋 Hi queziaa, welcome to swanlab!\n",
    "swanlab: Syncing run ./lora to the cloud\n",
    "swanlab: 🏠 View project at https://swanlab.cn/@queziaa/llm\n",
    "swanlab: 🚀 View run at https://swanlab.cn/@queziaa/llm/runs/wro5sea6nk45o2t7iagcp\n",
    "  0%|                                                   | 0/220 [00:00<?, ?it/s]Unsloth: Will smartly offload gradients to save VRAM!\n",
    "{'loss': 3.4634, 'grad_norm': 38.771568298339844, 'learning_rate': 0.0, 'epoch': 0.0}\n",
    "{'loss': 3.5042, 'grad_norm': 41.453060150146484, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}\n",
    "{'loss': 2.9567, 'grad_norm': 19.71192741394043, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.01}\n",
    "{'loss': 2.5382, 'grad_norm': 11.835859298706055, 'learning_rate': 1.2e-05, 'epoch': 0.02}\n",
    "{'loss': 2.4089, 'grad_norm': 10.100958824157715, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.02}\n",
    "{'loss': 2.3191, 'grad_norm': 8.870731353759766, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
    "{'loss': 2.4639, 'grad_norm': 8.528709411621094, 'learning_rate': 1.990697674418605e-05, 'epoch': 0.03}\n",
    "{'loss': 2.5167, 'grad_norm': 8.285380363464355, 'learning_rate': 1.9813953488372094e-05, 'epoch': 0.04}\n",
    "{'loss': 2.4859, 'grad_norm': 8.897400856018066, 'learning_rate': 1.9720930232558142e-05, 'epoch': 0.04}\n",
    "{'loss': 2.1208, 'grad_norm': 8.291818618774414, 'learning_rate': 1.9627906976744187e-05, 'epoch': 0.05}\n",
    "{'loss': 2.3541, 'grad_norm': 7.8962883949279785, 'learning_rate': 1.9534883720930235e-05, 'epoch': 0.05}\n",
    "{'loss': 2.0825, 'grad_norm': 7.747880935668945, 'learning_rate': 1.944186046511628e-05, 'epoch': 0.05}\n",
    "{'loss': 2.131, 'grad_norm': 8.15078067779541, 'learning_rate': 1.9348837209302327e-05, 'epoch': 0.06}\n",
    "{'loss': 2.2286, 'grad_norm': 7.977635860443115, 'learning_rate': 1.9255813953488375e-05, 'epoch': 0.06}\n",
    "{'loss': 2.2981, 'grad_norm': 7.534566402435303, 'learning_rate': 1.916279069767442e-05, 'epoch': 0.07}\n",
    "{'loss': 2.381, 'grad_norm': 7.569671154022217, 'learning_rate': 1.9069767441860468e-05, 'epoch': 0.07}\n",
    "{'loss': 2.3044, 'grad_norm': 6.835496425628662, 'learning_rate': 1.8976744186046516e-05, 'epoch': 0.08}\n",
    "{'loss': 2.2174, 'grad_norm': 7.1590094566345215, 'learning_rate': 1.888372093023256e-05, 'epoch': 0.08}\n",
    "{'loss': 2.0625, 'grad_norm': 7.2852702140808105, 'learning_rate': 1.8790697674418605e-05, 'epoch': 0.09}\n",
    "{'loss': 2.1817, 'grad_norm': 7.423367500305176, 'learning_rate': 1.8697674418604653e-05, 'epoch': 0.09}\n",
    "{'loss': 2.1075, 'grad_norm': 6.900639057159424, 'learning_rate': 1.86046511627907e-05, 'epoch': 0.1}\n",
    "{'loss': 2.1088, 'grad_norm': 7.493927478790283, 'learning_rate': 1.8511627906976745e-05, 'epoch': 0.1}\n",
    "{'loss': 2.2964, 'grad_norm': 6.6729559898376465, 'learning_rate': 1.8418604651162793e-05, 'epoch': 0.1}\n",
    "{'loss': 2.1329, 'grad_norm': 7.001527309417725, 'learning_rate': 1.8325581395348838e-05, 'epoch': 0.11}\n",
    "{'loss': 2.3261, 'grad_norm': 6.79864501953125, 'learning_rate': 1.8232558139534886e-05, 'epoch': 0.11}\n",
    "{'loss': 2.0284, 'grad_norm': 7.056204319000244, 'learning_rate': 1.813953488372093e-05, 'epoch': 0.12}\n",
    "{'loss': 2.1135, 'grad_norm': 7.194427013397217, 'learning_rate': 1.8046511627906978e-05, 'epoch': 0.12}\n",
    "{'loss': 1.9379, 'grad_norm': 6.986997604370117, 'learning_rate': 1.7953488372093023e-05, 'epoch': 0.13}\n",
    "{'loss': 2.0201, 'grad_norm': 7.062755107879639, 'learning_rate': 1.786046511627907e-05, 'epoch': 0.13}\n",
    "{'loss': 2.1211, 'grad_norm': 6.811195373535156, 'learning_rate': 1.776744186046512e-05, 'epoch': 0.14}\n",
    "{'loss': 2.3409, 'grad_norm': 6.945729732513428, 'learning_rate': 1.7674418604651163e-05, 'epoch': 0.14}\n",
    "{'loss': 2.3048, 'grad_norm': 6.856496334075928, 'learning_rate': 1.758139534883721e-05, 'epoch': 0.15}\n",
    "{'loss': 2.0986, 'grad_norm': 6.9074578285217285, 'learning_rate': 1.748837209302326e-05, 'epoch': 0.15}\n",
    "{'loss': 2.0035, 'grad_norm': 7.025321960449219, 'learning_rate': 1.7395348837209304e-05, 'epoch': 0.15}\n",
    "{'loss': 2.1933, 'grad_norm': 7.028587818145752, 'learning_rate': 1.7302325581395348e-05, 'epoch': 0.16}\n",
    "{'loss': 2.2341, 'grad_norm': 6.905514717102051, 'learning_rate': 1.7209302325581396e-05, 'epoch': 0.16}\n",
    "{'loss': 2.1176, 'grad_norm': 7.270166873931885, 'learning_rate': 1.7116279069767444e-05, 'epoch': 0.17}\n",
    "{'loss': 2.0756, 'grad_norm': 6.554558753967285, 'learning_rate': 1.702325581395349e-05, 'epoch': 0.17}\n",
    "{'loss': 1.8781, 'grad_norm': 6.422479629516602, 'learning_rate': 1.6930232558139537e-05, 'epoch': 0.18}\n",
    "{'loss': 1.9425, 'grad_norm': 6.449459075927734, 'learning_rate': 1.6837209302325585e-05, 'epoch': 0.18}\n",
    "{'loss': 2.0457, 'grad_norm': 6.794497013092041, 'learning_rate': 1.674418604651163e-05, 'epoch': 0.19}\n",
    "{'loss': 2.0385, 'grad_norm': 7.204444885253906, 'learning_rate': 1.6651162790697674e-05, 'epoch': 0.19}\n",
    "{'loss': 2.12, 'grad_norm': 6.825480937957764, 'learning_rate': 1.6558139534883722e-05, 'epoch': 0.2}\n",
    "{'loss': 2.151, 'grad_norm': 6.802484035491943, 'learning_rate': 1.646511627906977e-05, 'epoch': 0.2}\n",
    "{'loss': 2.1947, 'grad_norm': 6.612712383270264, 'learning_rate': 1.6372093023255814e-05, 'epoch': 0.2}\n",
    "{'loss': 1.9573, 'grad_norm': 6.716198921203613, 'learning_rate': 1.6279069767441862e-05, 'epoch': 0.21}\n",
    "{'loss': 2.1683, 'grad_norm': 7.113717555999756, 'learning_rate': 1.618604651162791e-05, 'epoch': 0.21}\n",
    "{'loss': 2.146, 'grad_norm': 7.115972995758057, 'learning_rate': 1.6093023255813955e-05, 'epoch': 0.22}\n",
    "{'loss': 2.3045, 'grad_norm': 6.899785041809082, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.22}\n",
    "{'loss': 2.0458, 'grad_norm': 6.719340801239014, 'learning_rate': 1.5906976744186047e-05, 'epoch': 0.23}\n",
    "{'loss': 1.9626, 'grad_norm': 7.399666786193848, 'learning_rate': 1.5813953488372095e-05, 'epoch': 0.23}\n",
    "{'loss': 1.9092, 'grad_norm': 6.674355983734131, 'learning_rate': 1.572093023255814e-05, 'epoch': 0.24}\n",
    "{'loss': 1.9243, 'grad_norm': 7.845666408538818, 'learning_rate': 1.5627906976744188e-05, 'epoch': 0.24}\n",
    "{'loss': 2.0864, 'grad_norm': 6.625521183013916, 'learning_rate': 1.5534883720930232e-05, 'epoch': 0.25}\n",
    "{'loss': 1.8893, 'grad_norm': 6.788233280181885, 'learning_rate': 1.544186046511628e-05, 'epoch': 0.25}\n",
    "{'loss': 2.2834, 'grad_norm': 6.525241374969482, 'learning_rate': 1.5348837209302328e-05, 'epoch': 0.25}\n",
    "{'loss': 2.3754, 'grad_norm': 7.202336311340332, 'learning_rate': 1.5255813953488374e-05, 'epoch': 0.26}\n",
    "{'loss': 2.1145, 'grad_norm': 6.429769992828369, 'learning_rate': 1.5162790697674419e-05, 'epoch': 0.26}\n",
    "{'loss': 2.0594, 'grad_norm': 6.568032264709473, 'learning_rate': 1.5069767441860465e-05, 'epoch': 0.27}\n",
    "{'loss': 2.0539, 'grad_norm': 7.06861686706543, 'learning_rate': 1.4976744186046512e-05, 'epoch': 0.27}\n",
    "{'loss': 1.9772, 'grad_norm': 7.232706546783447, 'learning_rate': 1.488372093023256e-05, 'epoch': 0.28}\n",
    "{'loss': 2.0357, 'grad_norm': 6.541831016540527, 'learning_rate': 1.4790697674418606e-05, 'epoch': 0.28}\n",
    "{'loss': 1.9813, 'grad_norm': 6.896732807159424, 'learning_rate': 1.4697674418604652e-05, 'epoch': 0.29}\n",
    "{'loss': 2.2807, 'grad_norm': 6.250576496124268, 'learning_rate': 1.46046511627907e-05, 'epoch': 0.29}\n",
    "{'loss': 2.0663, 'grad_norm': 6.395239353179932, 'learning_rate': 1.4511627906976746e-05, 'epoch': 0.3}\n",
    "{'loss': 1.8335, 'grad_norm': 6.36332368850708, 'learning_rate': 1.441860465116279e-05, 'epoch': 0.3}\n",
    "{'loss': 1.9827, 'grad_norm': 6.553510665893555, 'learning_rate': 1.4325581395348837e-05, 'epoch': 0.3}\n",
    "{'loss': 2.181, 'grad_norm': 6.574430465698242, 'learning_rate': 1.4232558139534885e-05, 'epoch': 0.31}\n",
    "{'loss': 2.0039, 'grad_norm': 6.177295207977295, 'learning_rate': 1.4139534883720931e-05, 'epoch': 0.31}\n",
    "{'loss': 1.8649, 'grad_norm': 6.393944263458252, 'learning_rate': 1.4046511627906978e-05, 'epoch': 0.32}\n",
    "{'loss': 2.053, 'grad_norm': 6.358188152313232, 'learning_rate': 1.3953488372093025e-05, 'epoch': 0.32}\n",
    "{'loss': 2.3502, 'grad_norm': 6.541483402252197, 'learning_rate': 1.3860465116279072e-05, 'epoch': 0.33}\n",
    "{'loss': 2.0888, 'grad_norm': 7.416222095489502, 'learning_rate': 1.3767441860465118e-05, 'epoch': 0.33}\n",
    "{'loss': 2.2887, 'grad_norm': 6.847437858581543, 'learning_rate': 1.3674418604651163e-05, 'epoch': 0.34}\n",
    "{'loss': 2.0799, 'grad_norm': 7.134405136108398, 'learning_rate': 1.358139534883721e-05, 'epoch': 0.34}\n",
    "{'loss': 1.9811, 'grad_norm': 6.788980007171631, 'learning_rate': 1.3488372093023257e-05, 'epoch': 0.35}\n",
    "{'loss': 1.7619, 'grad_norm': 6.5628662109375, 'learning_rate': 1.3395348837209303e-05, 'epoch': 0.35}\n",
    "{'loss': 2.1375, 'grad_norm': 6.5749993324279785, 'learning_rate': 1.330232558139535e-05, 'epoch': 0.35}\n",
    "{'loss': 2.0262, 'grad_norm': 6.51911735534668, 'learning_rate': 1.3209302325581397e-05, 'epoch': 0.36}\n",
    "{'loss': 2.0708, 'grad_norm': 6.322673320770264, 'learning_rate': 1.3116279069767443e-05, 'epoch': 0.36}\n",
    "{'loss': 2.004, 'grad_norm': 6.694781303405762, 'learning_rate': 1.302325581395349e-05, 'epoch': 0.37}\n",
    "{'loss': 2.1523, 'grad_norm': 6.519841194152832, 'learning_rate': 1.2930232558139534e-05, 'epoch': 0.37}\n",
    "{'loss': 2.2687, 'grad_norm': 6.535273551940918, 'learning_rate': 1.2837209302325582e-05, 'epoch': 0.38}\n",
    "{'loss': 2.1792, 'grad_norm': 6.4828410148620605, 'learning_rate': 1.2744186046511629e-05, 'epoch': 0.38}\n",
    "{'loss': 2.1624, 'grad_norm': 6.9809088706970215, 'learning_rate': 1.2651162790697675e-05, 'epoch': 0.39}\n",
    "{'loss': 1.8542, 'grad_norm': 6.715129375457764, 'learning_rate': 1.2558139534883723e-05, 'epoch': 0.39}\n",
    "{'loss': 1.8833, 'grad_norm': 6.51384162902832, 'learning_rate': 1.2465116279069769e-05, 'epoch': 0.4}\n",
    "{'loss': 1.9894, 'grad_norm': 6.2700514793396, 'learning_rate': 1.2372093023255815e-05, 'epoch': 0.4}\n",
    "{'loss': 1.9181, 'grad_norm': 6.405484199523926, 'learning_rate': 1.2279069767441863e-05, 'epoch': 0.4}\n",
    "{'loss': 2.0284, 'grad_norm': 6.400740146636963, 'learning_rate': 1.2186046511627908e-05, 'epoch': 0.41}\n",
    "{'loss': 2.0592, 'grad_norm': 6.255108833312988, 'learning_rate': 1.2093023255813954e-05, 'epoch': 0.41}\n",
    "{'loss': 1.8834, 'grad_norm': 6.1304826736450195, 'learning_rate': 1.2e-05, 'epoch': 0.42}\n",
    "{'loss': 2.0244, 'grad_norm': 6.471555233001709, 'learning_rate': 1.1906976744186047e-05, 'epoch': 0.42}\n",
    "{'loss': 1.8825, 'grad_norm': 6.423551559448242, 'learning_rate': 1.1813953488372095e-05, 'epoch': 0.43}\n",
    "{'loss': 1.826, 'grad_norm': 6.716546535491943, 'learning_rate': 1.172093023255814e-05, 'epoch': 0.43}\n",
    "{'loss': 1.8435, 'grad_norm': 6.232632637023926, 'learning_rate': 1.1627906976744187e-05, 'epoch': 0.44}\n",
    "{'loss': 1.8202, 'grad_norm': 6.361762046813965, 'learning_rate': 1.1534883720930235e-05, 'epoch': 0.44}\n",
    "{'loss': 2.146, 'grad_norm': 6.528078079223633, 'learning_rate': 1.144186046511628e-05, 'epoch': 0.45}\n",
    "{'loss': 1.9299, 'grad_norm': 6.6241865158081055, 'learning_rate': 1.1348837209302326e-05, 'epoch': 0.45}\n",
    "{'loss': 1.9678, 'grad_norm': 6.856834888458252, 'learning_rate': 1.1255813953488372e-05, 'epoch': 0.45}\n",
    "{'loss': 2.0605, 'grad_norm': 6.462306976318359, 'learning_rate': 1.116279069767442e-05, 'epoch': 0.46}\n",
    "{'loss': 1.9732, 'grad_norm': 6.475158214569092, 'learning_rate': 1.1069767441860466e-05, 'epoch': 0.46}\n",
    "{'loss': 1.9702, 'grad_norm': 7.04938268661499, 'learning_rate': 1.0976744186046513e-05, 'epoch': 0.47}\n",
    "{'loss': 2.0586, 'grad_norm': 6.458005428314209, 'learning_rate': 1.088372093023256e-05, 'epoch': 0.47}\n",
    "{'loss': 1.9067, 'grad_norm': 6.637826442718506, 'learning_rate': 1.0790697674418607e-05, 'epoch': 0.48}\n",
    "{'loss': 1.9581, 'grad_norm': 6.93996000289917, 'learning_rate': 1.0697674418604651e-05, 'epoch': 0.48}\n",
    "{'loss': 1.7571, 'grad_norm': 6.67562198638916, 'learning_rate': 1.0604651162790698e-05, 'epoch': 0.49}\n",
    "{'loss': 2.0016, 'grad_norm': 6.252933979034424, 'learning_rate': 1.0511627906976744e-05, 'epoch': 0.49}\n",
    "{'loss': 1.8169, 'grad_norm': 7.062551498413086, 'learning_rate': 1.0418604651162792e-05, 'epoch': 0.5}\n",
    "{'loss': 2.0761, 'grad_norm': 6.910422325134277, 'learning_rate': 1.0325581395348838e-05, 'epoch': 0.5}\n",
    "{'loss': 2.1773, 'grad_norm': 7.219082832336426, 'learning_rate': 1.0232558139534884e-05, 'epoch': 0.5}\n",
    "{'loss': 2.175, 'grad_norm': 6.996193885803223, 'learning_rate': 1.0139534883720932e-05, 'epoch': 0.51}\n",
    "{'loss': 1.9934, 'grad_norm': 7.187159538269043, 'learning_rate': 1.0046511627906979e-05, 'epoch': 0.51}\n",
    "{'loss': 1.9413, 'grad_norm': 6.437427043914795, 'learning_rate': 9.953488372093025e-06, 'epoch': 0.52}\n",
    "{'loss': 1.9533, 'grad_norm': 7.04979944229126, 'learning_rate': 9.860465116279071e-06, 'epoch': 0.52}\n",
    "{'loss': 2.1575, 'grad_norm': 6.383550643920898, 'learning_rate': 9.767441860465117e-06, 'epoch': 0.53}\n",
    "{'loss': 2.0519, 'grad_norm': 6.36712646484375, 'learning_rate': 9.674418604651164e-06, 'epoch': 0.53}\n",
    "{'loss': 1.8638, 'grad_norm': 6.402823448181152, 'learning_rate': 9.58139534883721e-06, 'epoch': 0.54}\n",
    "{'loss': 2.0565, 'grad_norm': 6.3555216789245605, 'learning_rate': 9.488372093023258e-06, 'epoch': 0.54}\n",
    "{'loss': 2.1146, 'grad_norm': 6.483825206756592, 'learning_rate': 9.395348837209302e-06, 'epoch': 0.55}\n",
    "{'loss': 1.9561, 'grad_norm': 6.444864273071289, 'learning_rate': 9.30232558139535e-06, 'epoch': 0.55}\n",
    "{'loss': 1.9729, 'grad_norm': 6.5276079177856445, 'learning_rate': 9.209302325581397e-06, 'epoch': 0.55}\n",
    "{'loss': 2.1872, 'grad_norm': 6.5129499435424805, 'learning_rate': 9.116279069767443e-06, 'epoch': 0.56}\n",
    "{'loss': 2.2468, 'grad_norm': 6.08482551574707, 'learning_rate': 9.023255813953489e-06, 'epoch': 0.56}\n",
    "{'loss': 1.9284, 'grad_norm': 6.549073696136475, 'learning_rate': 8.930232558139535e-06, 'epoch': 0.57}\n",
    "{'loss': 1.8427, 'grad_norm': 6.246048450469971, 'learning_rate': 8.837209302325582e-06, 'epoch': 0.57}\n",
    "{'loss': 2.0769, 'grad_norm': 6.722562313079834, 'learning_rate': 8.74418604651163e-06, 'epoch': 0.58}\n",
    "{'loss': 1.8762, 'grad_norm': 6.71519660949707, 'learning_rate': 8.651162790697674e-06, 'epoch': 0.58}\n",
    "{'loss': 1.9497, 'grad_norm': 6.66839599609375, 'learning_rate': 8.558139534883722e-06, 'epoch': 0.59}\n",
    "{'loss': 1.9743, 'grad_norm': 6.6872382164001465, 'learning_rate': 8.465116279069768e-06, 'epoch': 0.59}\n",
    "{'loss': 2.0045, 'grad_norm': 6.529005527496338, 'learning_rate': 8.372093023255815e-06, 'epoch': 0.6}\n",
    "{'loss': 2.0557, 'grad_norm': 6.5793561935424805, 'learning_rate': 8.279069767441861e-06, 'epoch': 0.6}\n",
    "{'loss': 1.9798, 'grad_norm': 6.595307350158691, 'learning_rate': 8.186046511627907e-06, 'epoch': 0.6}\n",
    "{'loss': 1.893, 'grad_norm': 6.453907489776611, 'learning_rate': 8.093023255813955e-06, 'epoch': 0.61}\n",
    "{'loss': 2.1912, 'grad_norm': 6.6888203620910645, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.61}\n",
    "{'loss': 1.7446, 'grad_norm': 6.486355781555176, 'learning_rate': 7.906976744186048e-06, 'epoch': 0.62}\n",
    "{'loss': 1.8635, 'grad_norm': 6.161719799041748, 'learning_rate': 7.813953488372094e-06, 'epoch': 0.62}\n",
    "{'loss': 1.9168, 'grad_norm': 6.469785213470459, 'learning_rate': 7.72093023255814e-06, 'epoch': 0.63}\n",
    "{'loss': 1.811, 'grad_norm': 6.797460079193115, 'learning_rate': 7.627906976744187e-06, 'epoch': 0.63}\n",
    "{'loss': 1.8297, 'grad_norm': 6.498678684234619, 'learning_rate': 7.534883720930233e-06, 'epoch': 0.64}\n",
    "{'loss': 1.929, 'grad_norm': 6.938605308532715, 'learning_rate': 7.44186046511628e-06, 'epoch': 0.64}\n",
    "{'loss': 2.0189, 'grad_norm': 6.074304580688477, 'learning_rate': 7.348837209302326e-06, 'epoch': 0.65}\n",
    "{'loss': 1.9044, 'grad_norm': 6.335511684417725, 'learning_rate': 7.255813953488373e-06, 'epoch': 0.65}\n",
    "{'loss': 1.8928, 'grad_norm': 6.5480852127075195, 'learning_rate': 7.1627906976744185e-06, 'epoch': 0.65}\n",
    "{'loss': 1.8804, 'grad_norm': 6.693275451660156, 'learning_rate': 7.069767441860466e-06, 'epoch': 0.66}\n",
    "{'loss': 1.9683, 'grad_norm': 6.448676109313965, 'learning_rate': 6.976744186046513e-06, 'epoch': 0.66}\n",
    "{'loss': 1.7707, 'grad_norm': 6.593653202056885, 'learning_rate': 6.883720930232559e-06, 'epoch': 0.67}\n",
    "{'loss': 1.8953, 'grad_norm': 6.677992343902588, 'learning_rate': 6.790697674418605e-06, 'epoch': 0.67}\n",
    "{'loss': 1.7718, 'grad_norm': 6.976492881774902, 'learning_rate': 6.6976744186046515e-06, 'epoch': 0.68}\n",
    "{'loss': 1.8769, 'grad_norm': 6.47564172744751, 'learning_rate': 6.604651162790699e-06, 'epoch': 0.68}\n",
    "{'loss': 2.0706, 'grad_norm': 6.441873073577881, 'learning_rate': 6.511627906976745e-06, 'epoch': 0.69}\n",
    "{'loss': 1.7572, 'grad_norm': 6.304922103881836, 'learning_rate': 6.418604651162791e-06, 'epoch': 0.69}\n",
    "{'loss': 2.2731, 'grad_norm': 6.318175315856934, 'learning_rate': 6.325581395348837e-06, 'epoch': 0.7}\n",
    "{'loss': 1.8799, 'grad_norm': 6.1689043045043945, 'learning_rate': 6.2325581395348845e-06, 'epoch': 0.7}\n",
    "{'loss': 2.1133, 'grad_norm': 6.4764814376831055, 'learning_rate': 6.139534883720932e-06, 'epoch': 0.7}\n",
    "{'loss': 2.3034, 'grad_norm': 6.470117568969727, 'learning_rate': 6.046511627906977e-06, 'epoch': 0.71}\n",
    "{'loss': 1.7268, 'grad_norm': 6.194000244140625, 'learning_rate': 5.953488372093023e-06, 'epoch': 0.71}\n",
    "{'loss': 2.1095, 'grad_norm': 6.3591718673706055, 'learning_rate': 5.86046511627907e-06, 'epoch': 0.72}\n",
    "{'loss': 2.0039, 'grad_norm': 6.301794528961182, 'learning_rate': 5.7674418604651175e-06, 'epoch': 0.72}\n",
    "{'loss': 2.1649, 'grad_norm': 6.518779754638672, 'learning_rate': 5.674418604651163e-06, 'epoch': 0.73}\n",
    "{'loss': 2.1493, 'grad_norm': 6.735535621643066, 'learning_rate': 5.58139534883721e-06, 'epoch': 0.73}\n",
    "{'loss': 1.853, 'grad_norm': 6.409810543060303, 'learning_rate': 5.488372093023256e-06, 'epoch': 0.74}\n",
    "{'loss': 1.6337, 'grad_norm': 6.446136474609375, 'learning_rate': 5.395348837209303e-06, 'epoch': 0.74}\n",
    "{'loss': 1.8919, 'grad_norm': 6.459985733032227, 'learning_rate': 5.302325581395349e-06, 'epoch': 0.75}\n",
    "{'loss': 1.8559, 'grad_norm': 6.422778129577637, 'learning_rate': 5.209302325581396e-06, 'epoch': 0.75}\n",
    "{'loss': 2.0196, 'grad_norm': 6.101391792297363, 'learning_rate': 5.116279069767442e-06, 'epoch': 0.75}\n",
    "{'loss': 2.038, 'grad_norm': 6.887827396392822, 'learning_rate': 5.023255813953489e-06, 'epoch': 0.76}\n",
    "{'loss': 2.028, 'grad_norm': 6.082711219787598, 'learning_rate': 4.9302325581395355e-06, 'epoch': 0.76}\n",
    "{'loss': 1.9091, 'grad_norm': 6.189083099365234, 'learning_rate': 4.837209302325582e-06, 'epoch': 0.77}\n",
    "{'loss': 1.8371, 'grad_norm': 6.3344807624816895, 'learning_rate': 4.744186046511629e-06, 'epoch': 0.77}\n",
    "{'loss': 1.8946, 'grad_norm': 6.416741371154785, 'learning_rate': 4.651162790697675e-06, 'epoch': 0.78}\n",
    "{'loss': 1.8916, 'grad_norm': 6.499812126159668, 'learning_rate': 4.558139534883721e-06, 'epoch': 0.78}\n",
    "{'loss': 1.7531, 'grad_norm': 5.983631610870361, 'learning_rate': 4.465116279069768e-06, 'epoch': 0.79}\n",
    "{'loss': 2.0808, 'grad_norm': 6.169745922088623, 'learning_rate': 4.372093023255815e-06, 'epoch': 0.79}\n",
    "{'loss': 2.255, 'grad_norm': 6.35945463180542, 'learning_rate': 4.279069767441861e-06, 'epoch': 0.8}\n",
    "{'loss': 2.1389, 'grad_norm': 6.436844348907471, 'learning_rate': 4.186046511627907e-06, 'epoch': 0.8}\n",
    "{'loss': 1.604, 'grad_norm': 6.42203426361084, 'learning_rate': 4.0930232558139536e-06, 'epoch': 0.8}\n",
    "{'loss': 1.8176, 'grad_norm': 6.3183135986328125, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.81}\n",
    "{'loss': 1.7403, 'grad_norm': 6.241849422454834, 'learning_rate': 3.906976744186047e-06, 'epoch': 0.81}\n",
    "{'loss': 1.9908, 'grad_norm': 6.297408103942871, 'learning_rate': 3.8139534883720936e-06, 'epoch': 0.82}\n",
    "{'loss': 1.8668, 'grad_norm': 6.540995121002197, 'learning_rate': 3.72093023255814e-06, 'epoch': 0.82}\n",
    "{'loss': 2.0863, 'grad_norm': 6.494701385498047, 'learning_rate': 3.6279069767441866e-06, 'epoch': 0.83}\n",
    "{'loss': 2.059, 'grad_norm': 6.524003982543945, 'learning_rate': 3.534883720930233e-06, 'epoch': 0.83}\n",
    "{'loss': 2.1939, 'grad_norm': 6.675126552581787, 'learning_rate': 3.4418604651162795e-06, 'epoch': 0.84}\n",
    "{'loss': 2.009, 'grad_norm': 6.563803195953369, 'learning_rate': 3.3488372093023258e-06, 'epoch': 0.84}\n",
    "{'loss': 1.916, 'grad_norm': 6.233925819396973, 'learning_rate': 3.2558139534883724e-06, 'epoch': 0.85}\n",
    "{'loss': 1.9958, 'grad_norm': 6.7176899909973145, 'learning_rate': 3.1627906976744187e-06, 'epoch': 0.85}\n",
    "{'loss': 2.0033, 'grad_norm': 6.530955791473389, 'learning_rate': 3.069767441860466e-06, 'epoch': 0.85}\n",
    "{'loss': 2.0454, 'grad_norm': 6.700470447540283, 'learning_rate': 2.9767441860465116e-06, 'epoch': 0.86}\n",
    "{'loss': 1.9093, 'grad_norm': 6.9345526695251465, 'learning_rate': 2.8837209302325587e-06, 'epoch': 0.86}\n",
    "{'loss': 1.6962, 'grad_norm': 6.1845383644104, 'learning_rate': 2.790697674418605e-06, 'epoch': 0.87}\n",
    "{'loss': 1.7568, 'grad_norm': 6.218188762664795, 'learning_rate': 2.6976744186046517e-06, 'epoch': 0.87}\n",
    "{'loss': 2.0486, 'grad_norm': 6.48583984375, 'learning_rate': 2.604651162790698e-06, 'epoch': 0.88}\n",
    "{'loss': 1.8909, 'grad_norm': 6.175060749053955, 'learning_rate': 2.5116279069767446e-06, 'epoch': 0.88}\n",
    "{'loss': 2.1503, 'grad_norm': 6.681482791900635, 'learning_rate': 2.418604651162791e-06, 'epoch': 0.89}\n",
    "{'loss': 2.1893, 'grad_norm': 6.523087024688721, 'learning_rate': 2.3255813953488376e-06, 'epoch': 0.89}\n",
    "{'loss': 2.0756, 'grad_norm': 6.303102493286133, 'learning_rate': 2.232558139534884e-06, 'epoch': 0.9}\n",
    "{'loss': 1.7711, 'grad_norm': 6.259105205535889, 'learning_rate': 2.1395348837209305e-06, 'epoch': 0.9}\n",
    "{'loss': 1.7003, 'grad_norm': 6.42082405090332, 'learning_rate': 2.0465116279069768e-06, 'epoch': 0.9}\n",
    "{'loss': 1.8458, 'grad_norm': 6.243340492248535, 'learning_rate': 1.9534883720930235e-06, 'epoch': 0.91}\n",
    "{'loss': 2.0195, 'grad_norm': 6.454085350036621, 'learning_rate': 1.86046511627907e-06, 'epoch': 0.91}\n",
    "{'loss': 2.5074, 'grad_norm': 6.549962520599365, 'learning_rate': 1.7674418604651164e-06, 'epoch': 0.92}\n",
    "{'loss': 1.8392, 'grad_norm': 6.391505241394043, 'learning_rate': 1.6744186046511629e-06, 'epoch': 0.92}\n",
    "{'loss': 1.7812, 'grad_norm': 6.7058610916137695, 'learning_rate': 1.5813953488372093e-06, 'epoch': 0.93}\n",
    "{'loss': 2.0071, 'grad_norm': 6.800440788269043, 'learning_rate': 1.4883720930232558e-06, 'epoch': 0.93}\n",
    "{'loss': 1.7881, 'grad_norm': 6.137300968170166, 'learning_rate': 1.3953488372093025e-06, 'epoch': 0.94}\n",
    "{'loss': 2.2735, 'grad_norm': 6.3557963371276855, 'learning_rate': 1.302325581395349e-06, 'epoch': 0.94}\n",
    "{'loss': 1.9379, 'grad_norm': 6.398797035217285, 'learning_rate': 1.2093023255813954e-06, 'epoch': 0.95}\n",
    "{'loss': 1.8923, 'grad_norm': 6.311578273773193, 'learning_rate': 1.116279069767442e-06, 'epoch': 0.95}\n",
    "{'loss': 1.9321, 'grad_norm': 6.293050765991211, 'learning_rate': 1.0232558139534884e-06, 'epoch': 0.95}\n",
    "{'loss': 2.0063, 'grad_norm': 6.363559246063232, 'learning_rate': 9.30232558139535e-07, 'epoch': 0.96}\n",
    "{'loss': 1.8112, 'grad_norm': 7.25993013381958, 'learning_rate': 8.372093023255814e-07, 'epoch': 0.96}\n",
    "{'loss': 2.049, 'grad_norm': 6.48196268081665, 'learning_rate': 7.441860465116279e-07, 'epoch': 0.97}\n",
    "{'loss': 2.0512, 'grad_norm': 6.4935431480407715, 'learning_rate': 6.511627906976745e-07, 'epoch': 0.97}\n",
    "{'loss': 2.0611, 'grad_norm': 5.975673198699951, 'learning_rate': 5.58139534883721e-07, 'epoch': 0.98}\n",
    "{'loss': 1.8955, 'grad_norm': 6.01713228225708, 'learning_rate': 4.651162790697675e-07, 'epoch': 0.98}\n",
    "{'loss': 1.931, 'grad_norm': 6.487911224365234, 'learning_rate': 3.7209302325581396e-07, 'epoch': 0.99}\n",
    "{'loss': 1.702, 'grad_norm': 6.380808353424072, 'learning_rate': 2.790697674418605e-07, 'epoch': 0.99}\n",
    "{'loss': 2.0689, 'grad_norm': 6.218726634979248, 'learning_rate': 1.8604651162790698e-07, 'epoch': 1.0}\n",
    "{'loss': 1.9194, 'grad_norm': 7.072579383850098, 'learning_rate': 9.302325581395349e-08, 'epoch': 1.0}\n",
    "100%|█████████████████████████████████████████| 220/220 [06:36<00:00,  1.90s/it]swanlab: Step 220 on key train/epoch already exists, ignored.\n",
    "swanlab: Step 220 on key train/global_step already exists, ignored.\n",
    "{'train_runtime': 403.0117, 'train_samples_per_second': 8.727, 'train_steps_per_second': 0.546, 'train_loss': 2.0447323192249645, 'epoch': 1.0}\n",
    "100%|█████████████████████████████████████████| 220/220 [06:41<00:00,  1.83s/it]\n",
    "Renamed ./lora/checkpoint-220 to ./12outputnew3CC\n",
    "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.                           \n",
    "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
    "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
    "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.51.3.\n",
    "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 22.159 GB. Platform: Linux.\n",
    "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
    "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
    " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
    "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
    "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.39it/s]\n",
    "Unsloth 2025.3.19 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n",
    "Unsloth: Tokenizing [\"prompt\"] (num_proc=2): 100%|█| 2709/2709 [00:01<00:00, 171\n",
    "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
    "   \\\\   /|    Num examples = 2,709 | Num Epochs = 1 | Total steps = 169\n",
    "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
    "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
    " \"-____-\"     Trainable parameters = 322,961,408/7,000,000,000 (4.61% trained)\n",
    "swanlab: Tracking run with swanlab version 0.5.5                                   \n",
    "swanlab: Run data will be saved locally in /llm/swanlog/run-20250420_155658-0e8cd89d\n",
    "swanlab: 👋 Hi queziaa, welcome to swanlab!\n",
    "swanlab: Syncing run ./lora to the cloud\n",
    "swanlab: 🏠 View project at https://swanlab.cn/@queziaa/llm\n",
    "swanlab: 🚀 View run at https://swanlab.cn/@queziaa/llm/runs/1wivcymfeu0dmaaqe6lgl\n",
    "  0%|                                                   | 0/169 [00:00<?, ?it/s]Unsloth: Will smartly offload gradients to save VRAM!\n",
    "{'loss': 2.0817, 'grad_norm': 6.961860656738281, 'learning_rate': 0.0, 'epoch': 0.01}\n",
    "{'loss': 2.3145, 'grad_norm': 7.235713481903076, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}\n",
    "{'loss': 2.2499, 'grad_norm': 7.195122718811035, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.02}\n",
    "{'loss': 2.2615, 'grad_norm': 7.11149263381958, 'learning_rate': 1.2e-05, 'epoch': 0.02}\n",
    "{'loss': 2.2839, 'grad_norm': 6.873345851898193, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.03}\n",
    "{'loss': 2.1435, 'grad_norm': 7.487974166870117, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
    "{'loss': 2.1715, 'grad_norm': 7.666033744812012, 'learning_rate': 1.9878048780487806e-05, 'epoch': 0.04}\n",
    "{'loss': 2.0501, 'grad_norm': 7.452733039855957, 'learning_rate': 1.975609756097561e-05, 'epoch': 0.05}\n",
    "{'loss': 2.2283, 'grad_norm': 7.523535251617432, 'learning_rate': 1.9634146341463414e-05, 'epoch': 0.05}\n",
    "{'loss': 2.0229, 'grad_norm': 7.295567989349365, 'learning_rate': 1.9512195121951222e-05, 'epoch': 0.06}\n",
    "{'loss': 2.4333, 'grad_norm': 8.460769653320312, 'learning_rate': 1.9390243902439026e-05, 'epoch': 0.06}\n",
    "{'loss': 2.1289, 'grad_norm': 7.306964874267578, 'learning_rate': 1.926829268292683e-05, 'epoch': 0.07}\n",
    "{'loss': 2.2898, 'grad_norm': 7.46250057220459, 'learning_rate': 1.9146341463414635e-05, 'epoch': 0.08}\n",
    "{'loss': 2.1158, 'grad_norm': 7.009105205535889, 'learning_rate': 1.902439024390244e-05, 'epoch': 0.08}\n",
    "{'loss': 2.103, 'grad_norm': 7.064920902252197, 'learning_rate': 1.8902439024390243e-05, 'epoch': 0.09}\n",
    "{'loss': 2.0018, 'grad_norm': 7.248971939086914, 'learning_rate': 1.878048780487805e-05, 'epoch': 0.09}\n",
    "{'loss': 2.3169, 'grad_norm': 7.31209659576416, 'learning_rate': 1.8658536585365855e-05, 'epoch': 0.1}\n",
    "{'loss': 2.332, 'grad_norm': 6.96775484085083, 'learning_rate': 1.8536585365853663e-05, 'epoch': 0.11}\n",
    "{'loss': 2.165, 'grad_norm': 7.1296305656433105, 'learning_rate': 1.8414634146341467e-05, 'epoch': 0.11}\n",
    "{'loss': 2.0999, 'grad_norm': 7.188024044036865, 'learning_rate': 1.829268292682927e-05, 'epoch': 0.12}\n",
    "{'loss': 2.2135, 'grad_norm': 7.763185977935791, 'learning_rate': 1.8170731707317075e-05, 'epoch': 0.12}\n",
    "{'loss': 2.0889, 'grad_norm': 7.194012641906738, 'learning_rate': 1.804878048780488e-05, 'epoch': 0.13}\n",
    "{'loss': 2.2928, 'grad_norm': 7.3077263832092285, 'learning_rate': 1.7926829268292684e-05, 'epoch': 0.14}\n",
    "{'loss': 1.9642, 'grad_norm': 7.535406112670898, 'learning_rate': 1.7804878048780488e-05, 'epoch': 0.14}\n",
    "{'loss': 2.0799, 'grad_norm': 7.501422882080078, 'learning_rate': 1.7682926829268296e-05, 'epoch': 0.15}\n",
    "{'loss': 2.014, 'grad_norm': 7.800945281982422, 'learning_rate': 1.75609756097561e-05, 'epoch': 0.15}\n",
    "{'loss': 2.1116, 'grad_norm': 7.776002407073975, 'learning_rate': 1.7439024390243904e-05, 'epoch': 0.16}\n",
    "{'loss': 2.3022, 'grad_norm': 7.0010528564453125, 'learning_rate': 1.7317073170731708e-05, 'epoch': 0.17}\n",
    "{'loss': 2.1443, 'grad_norm': 7.858713150024414, 'learning_rate': 1.7195121951219512e-05, 'epoch': 0.17}\n",
    "{'loss': 2.0012, 'grad_norm': 7.181500434875488, 'learning_rate': 1.7073170731707317e-05, 'epoch': 0.18}\n",
    "{'loss': 2.3047, 'grad_norm': 7.249805450439453, 'learning_rate': 1.6951219512195124e-05, 'epoch': 0.18}\n",
    "{'loss': 2.1682, 'grad_norm': 7.606344223022461, 'learning_rate': 1.682926829268293e-05, 'epoch': 0.19}\n",
    "{'loss': 2.0421, 'grad_norm': 6.892155647277832, 'learning_rate': 1.6707317073170733e-05, 'epoch': 0.19}\n",
    "{'loss': 1.9831, 'grad_norm': 7.6543192863464355, 'learning_rate': 1.6585365853658537e-05, 'epoch': 0.2}\n",
    "{'loss': 2.1301, 'grad_norm': 7.1684675216674805, 'learning_rate': 1.646341463414634e-05, 'epoch': 0.21}\n",
    "{'loss': 1.9498, 'grad_norm': 7.190018177032471, 'learning_rate': 1.6341463414634145e-05, 'epoch': 0.21}\n",
    "{'loss': 2.0785, 'grad_norm': 7.368025779724121, 'learning_rate': 1.6219512195121953e-05, 'epoch': 0.22}\n",
    "{'loss': 1.8856, 'grad_norm': 7.001066207885742, 'learning_rate': 1.6097560975609757e-05, 'epoch': 0.22}\n",
    "{'loss': 2.1023, 'grad_norm': 7.62183952331543, 'learning_rate': 1.597560975609756e-05, 'epoch': 0.23}\n",
    "{'loss': 2.0382, 'grad_norm': 7.350465297698975, 'learning_rate': 1.585365853658537e-05, 'epoch': 0.24}\n",
    "{'loss': 2.0592, 'grad_norm': 7.6837968826293945, 'learning_rate': 1.5731707317073173e-05, 'epoch': 0.24}\n",
    "{'loss': 2.0371, 'grad_norm': 7.4954705238342285, 'learning_rate': 1.5609756097560978e-05, 'epoch': 0.25}\n",
    "{'loss': 2.0349, 'grad_norm': 7.889945030212402, 'learning_rate': 1.5487804878048782e-05, 'epoch': 0.25}\n",
    "{'loss': 2.1574, 'grad_norm': 7.690540790557861, 'learning_rate': 1.5365853658536586e-05, 'epoch': 0.26}\n",
    "{'loss': 2.1298, 'grad_norm': 7.558493137359619, 'learning_rate': 1.5243902439024392e-05, 'epoch': 0.27}\n",
    "{'loss': 2.1859, 'grad_norm': 7.678737640380859, 'learning_rate': 1.5121951219512196e-05, 'epoch': 0.27}\n",
    "{'loss': 2.1761, 'grad_norm': 7.373955726623535, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.28}\n",
    "{'loss': 2.0282, 'grad_norm': 7.610326290130615, 'learning_rate': 1.4878048780487806e-05, 'epoch': 0.28}\n",
    "{'loss': 2.3775, 'grad_norm': 7.868314743041992, 'learning_rate': 1.475609756097561e-05, 'epoch': 0.29}\n",
    "{'loss': 2.0511, 'grad_norm': 7.283028602600098, 'learning_rate': 1.4634146341463415e-05, 'epoch': 0.29}\n",
    "{'loss': 1.8966, 'grad_norm': 7.4138712882995605, 'learning_rate': 1.451219512195122e-05, 'epoch': 0.3}\n",
    "{'loss': 2.2314, 'grad_norm': 7.528009414672852, 'learning_rate': 1.4390243902439025e-05, 'epoch': 0.31}\n",
    "{'loss': 2.0464, 'grad_norm': 7.3993449211120605, 'learning_rate': 1.4268292682926829e-05, 'epoch': 0.31}\n",
    "{'loss': 2.0456, 'grad_norm': 7.342483997344971, 'learning_rate': 1.4146341463414635e-05, 'epoch': 0.32}\n",
    "{'loss': 2.1909, 'grad_norm': 8.084850311279297, 'learning_rate': 1.402439024390244e-05, 'epoch': 0.32}\n",
    "{'loss': 1.8631, 'grad_norm': 7.711967945098877, 'learning_rate': 1.3902439024390244e-05, 'epoch': 0.33}\n",
    "{'loss': 1.9734, 'grad_norm': 7.568871021270752, 'learning_rate': 1.378048780487805e-05, 'epoch': 0.34}\n",
    "{'loss': 2.2077, 'grad_norm': 7.590179920196533, 'learning_rate': 1.3658536585365855e-05, 'epoch': 0.34}\n",
    "{'loss': 2.0104, 'grad_norm': 7.208364486694336, 'learning_rate': 1.3536585365853661e-05, 'epoch': 0.35}\n",
    "{'loss': 2.0134, 'grad_norm': 7.666910648345947, 'learning_rate': 1.3414634146341466e-05, 'epoch': 0.35}\n",
    "{'loss': 2.2022, 'grad_norm': 7.560666561126709, 'learning_rate': 1.329268292682927e-05, 'epoch': 0.36}\n",
    "{'loss': 2.2134, 'grad_norm': 7.924431324005127, 'learning_rate': 1.3170731707317076e-05, 'epoch': 0.37}\n",
    "{'loss': 2.089, 'grad_norm': 7.073612689971924, 'learning_rate': 1.304878048780488e-05, 'epoch': 0.37}\n",
    "{'loss': 2.0596, 'grad_norm': 7.67110538482666, 'learning_rate': 1.2926829268292684e-05, 'epoch': 0.38}\n",
    "{'loss': 1.9958, 'grad_norm': 7.634411811828613, 'learning_rate': 1.2804878048780488e-05, 'epoch': 0.38}\n",
    "{'loss': 1.8782, 'grad_norm': 7.090981483459473, 'learning_rate': 1.2682926829268294e-05, 'epoch': 0.39}\n",
    "{'loss': 1.9226, 'grad_norm': 7.394128799438477, 'learning_rate': 1.2560975609756098e-05, 'epoch': 0.4}\n",
    "{'loss': 1.87, 'grad_norm': 7.310108661651611, 'learning_rate': 1.2439024390243903e-05, 'epoch': 0.4}\n",
    "{'loss': 2.0432, 'grad_norm': 7.4974517822265625, 'learning_rate': 1.2317073170731709e-05, 'epoch': 0.41}\n",
    "{'loss': 1.7944, 'grad_norm': 7.547341346740723, 'learning_rate': 1.2195121951219513e-05, 'epoch': 0.41}\n",
    "{'loss': 2.1412, 'grad_norm': 8.332528114318848, 'learning_rate': 1.2073170731707317e-05, 'epoch': 0.42}\n",
    "{'loss': 1.744, 'grad_norm': 7.94748067855835, 'learning_rate': 1.1951219512195123e-05, 'epoch': 0.42}\n",
    "{'loss': 2.3303, 'grad_norm': 8.286905288696289, 'learning_rate': 1.1829268292682927e-05, 'epoch': 0.43}\n",
    "{'loss': 1.9144, 'grad_norm': 7.682699680328369, 'learning_rate': 1.1707317073170731e-05, 'epoch': 0.44}\n",
    "{'loss': 1.9186, 'grad_norm': 7.492463111877441, 'learning_rate': 1.1585365853658537e-05, 'epoch': 0.44}\n",
    "{'loss': 1.9217, 'grad_norm': 7.558982849121094, 'learning_rate': 1.1463414634146342e-05, 'epoch': 0.45}\n",
    "{'loss': 2.1744, 'grad_norm': 7.939598560333252, 'learning_rate': 1.1341463414634146e-05, 'epoch': 0.45}\n",
    "{'loss': 1.9915, 'grad_norm': 8.297235488891602, 'learning_rate': 1.1219512195121953e-05, 'epoch': 0.46}\n",
    "{'loss': 1.8373, 'grad_norm': 7.422395706176758, 'learning_rate': 1.1097560975609758e-05, 'epoch': 0.47}\n",
    "{'loss': 1.954, 'grad_norm': 7.054702281951904, 'learning_rate': 1.0975609756097562e-05, 'epoch': 0.47}\n",
    "{'loss': 2.22, 'grad_norm': 7.379438877105713, 'learning_rate': 1.0853658536585368e-05, 'epoch': 0.48}\n",
    "{'loss': 2.258, 'grad_norm': 8.162018775939941, 'learning_rate': 1.0731707317073172e-05, 'epoch': 0.48}\n",
    "{'loss': 2.2344, 'grad_norm': 7.688046932220459, 'learning_rate': 1.0609756097560976e-05, 'epoch': 0.49}\n",
    "{'loss': 2.0733, 'grad_norm': 7.4679718017578125, 'learning_rate': 1.0487804878048782e-05, 'epoch': 0.5}\n",
    "{'loss': 1.9615, 'grad_norm': 7.349484443664551, 'learning_rate': 1.0365853658536586e-05, 'epoch': 0.5}\n",
    "{'loss': 1.9663, 'grad_norm': 7.352814197540283, 'learning_rate': 1.024390243902439e-05, 'epoch': 0.51}\n",
    "{'loss': 2.0932, 'grad_norm': 7.979010105133057, 'learning_rate': 1.0121951219512197e-05, 'epoch': 0.51}\n",
    "{'loss': 1.8451, 'grad_norm': 7.5438923835754395, 'learning_rate': 1e-05, 'epoch': 0.52}\n",
    "{'loss': 2.1492, 'grad_norm': 7.670048236846924, 'learning_rate': 9.878048780487805e-06, 'epoch': 0.53}\n",
    "{'loss': 1.9424, 'grad_norm': 7.557637691497803, 'learning_rate': 9.756097560975611e-06, 'epoch': 0.53}\n",
    "{'loss': 2.137, 'grad_norm': 7.781534194946289, 'learning_rate': 9.634146341463415e-06, 'epoch': 0.54}\n",
    "{'loss': 2.1005, 'grad_norm': 8.082075119018555, 'learning_rate': 9.51219512195122e-06, 'epoch': 0.54}\n",
    "{'loss': 2.1206, 'grad_norm': 8.298896789550781, 'learning_rate': 9.390243902439025e-06, 'epoch': 0.55}\n",
    "{'loss': 2.1168, 'grad_norm': 7.660041332244873, 'learning_rate': 9.268292682926831e-06, 'epoch': 0.55}\n",
    "{'loss': 1.956, 'grad_norm': 7.4125823974609375, 'learning_rate': 9.146341463414635e-06, 'epoch': 0.56}\n",
    "{'loss': 1.9499, 'grad_norm': 7.6620049476623535, 'learning_rate': 9.02439024390244e-06, 'epoch': 0.57}\n",
    "{'loss': 2.2022, 'grad_norm': 8.15787124633789, 'learning_rate': 8.902439024390244e-06, 'epoch': 0.57}\n",
    "{'loss': 1.9503, 'grad_norm': 7.80277681350708, 'learning_rate': 8.78048780487805e-06, 'epoch': 0.58}\n",
    "{'loss': 2.4588, 'grad_norm': 8.978318214416504, 'learning_rate': 8.658536585365854e-06, 'epoch': 0.58}\n",
    "{'loss': 1.6919, 'grad_norm': 7.096734046936035, 'learning_rate': 8.536585365853658e-06, 'epoch': 0.59}\n",
    "{'loss': 1.9411, 'grad_norm': 7.915173530578613, 'learning_rate': 8.414634146341464e-06, 'epoch': 0.6}\n",
    "{'loss': 1.961, 'grad_norm': 8.089136123657227, 'learning_rate': 8.292682926829268e-06, 'epoch': 0.6}\n",
    "{'loss': 1.9989, 'grad_norm': 8.04111099243164, 'learning_rate': 8.170731707317073e-06, 'epoch': 0.61}\n",
    "{'loss': 1.9725, 'grad_norm': 7.872181415557861, 'learning_rate': 8.048780487804879e-06, 'epoch': 0.61}\n",
    "{'loss': 1.8578, 'grad_norm': 7.477814674377441, 'learning_rate': 7.926829268292685e-06, 'epoch': 0.62}\n",
    "{'loss': 1.9336, 'grad_norm': 7.4446120262146, 'learning_rate': 7.804878048780489e-06, 'epoch': 0.63}\n",
    "{'loss': 1.9626, 'grad_norm': 8.205106735229492, 'learning_rate': 7.682926829268293e-06, 'epoch': 0.63}\n",
    "{'loss': 1.8967, 'grad_norm': 7.2841596603393555, 'learning_rate': 7.560975609756098e-06, 'epoch': 0.64}\n",
    "{'loss': 1.7425, 'grad_norm': 7.826197624206543, 'learning_rate': 7.439024390243903e-06, 'epoch': 0.64}\n",
    "{'loss': 1.7595, 'grad_norm': 7.332771301269531, 'learning_rate': 7.317073170731707e-06, 'epoch': 0.65}\n",
    "{'loss': 1.7668, 'grad_norm': 7.5895514488220215, 'learning_rate': 7.1951219512195125e-06, 'epoch': 0.65}\n",
    "{'loss': 1.968, 'grad_norm': 7.5871100425720215, 'learning_rate': 7.0731707317073175e-06, 'epoch': 0.66}\n",
    "{'loss': 1.944, 'grad_norm': 8.586139678955078, 'learning_rate': 6.951219512195122e-06, 'epoch': 0.67}\n",
    "{'loss': 1.9772, 'grad_norm': 8.431906700134277, 'learning_rate': 6.829268292682928e-06, 'epoch': 0.67}\n",
    "{'loss': 2.118, 'grad_norm': 8.586480140686035, 'learning_rate': 6.707317073170733e-06, 'epoch': 0.68}\n",
    "{'loss': 1.8555, 'grad_norm': 8.096282958984375, 'learning_rate': 6.585365853658538e-06, 'epoch': 0.68}\n",
    "{'loss': 2.0312, 'grad_norm': 8.839920997619629, 'learning_rate': 6.463414634146342e-06, 'epoch': 0.69}\n",
    "{'loss': 1.9793, 'grad_norm': 8.602499008178711, 'learning_rate': 6.341463414634147e-06, 'epoch': 0.7}\n",
    "{'loss': 2.2653, 'grad_norm': 9.256917953491211, 'learning_rate': 6.219512195121951e-06, 'epoch': 0.7}\n",
    "{'loss': 1.9694, 'grad_norm': 8.537774085998535, 'learning_rate': 6.0975609756097564e-06, 'epoch': 0.71}\n",
    "{'loss': 2.027, 'grad_norm': 8.119497299194336, 'learning_rate': 5.9756097560975615e-06, 'epoch': 0.71}\n",
    "{'loss': 1.8938, 'grad_norm': 8.386564254760742, 'learning_rate': 5.853658536585366e-06, 'epoch': 0.72}\n",
    "{'loss': 1.7637, 'grad_norm': 7.624439239501953, 'learning_rate': 5.731707317073171e-06, 'epoch': 0.73}\n",
    "{'loss': 1.9367, 'grad_norm': 8.215943336486816, 'learning_rate': 5.609756097560977e-06, 'epoch': 0.73}\n",
    "{'loss': 1.9423, 'grad_norm': 7.786112308502197, 'learning_rate': 5.487804878048781e-06, 'epoch': 0.74}\n",
    "{'loss': 1.9027, 'grad_norm': 7.641944408416748, 'learning_rate': 5.365853658536586e-06, 'epoch': 0.74}\n",
    "{'loss': 1.8587, 'grad_norm': 8.037354469299316, 'learning_rate': 5.243902439024391e-06, 'epoch': 0.75}\n",
    "{'loss': 1.947, 'grad_norm': 7.719983100891113, 'learning_rate': 5.121951219512195e-06, 'epoch': 0.76}\n",
    "{'loss': 2.0555, 'grad_norm': 7.857635021209717, 'learning_rate': 5e-06, 'epoch': 0.76}\n",
    "{'loss': 1.8707, 'grad_norm': 7.743967056274414, 'learning_rate': 4.8780487804878055e-06, 'epoch': 0.77}\n",
    "{'loss': 2.17, 'grad_norm': 8.774515151977539, 'learning_rate': 4.75609756097561e-06, 'epoch': 0.77}\n",
    "{'loss': 1.8835, 'grad_norm': 7.631585121154785, 'learning_rate': 4.634146341463416e-06, 'epoch': 0.78}\n",
    "{'loss': 2.0444, 'grad_norm': 7.55304479598999, 'learning_rate': 4.51219512195122e-06, 'epoch': 0.78}\n",
    "{'loss': 2.23, 'grad_norm': 8.186344146728516, 'learning_rate': 4.390243902439025e-06, 'epoch': 0.79}\n",
    "{'loss': 2.0813, 'grad_norm': 8.090989112854004, 'learning_rate': 4.268292682926829e-06, 'epoch': 0.8}\n",
    "{'loss': 1.9401, 'grad_norm': 7.779296398162842, 'learning_rate': 4.146341463414634e-06, 'epoch': 0.8}\n",
    "{'loss': 1.8508, 'grad_norm': 7.411500453948975, 'learning_rate': 4.024390243902439e-06, 'epoch': 0.81}\n",
    "{'loss': 1.9861, 'grad_norm': 7.929643154144287, 'learning_rate': 3.902439024390244e-06, 'epoch': 0.81}\n",
    "{'loss': 1.8835, 'grad_norm': 7.995815753936768, 'learning_rate': 3.780487804878049e-06, 'epoch': 0.82}\n",
    "{'loss': 2.1119, 'grad_norm': 8.124931335449219, 'learning_rate': 3.6585365853658537e-06, 'epoch': 0.83}\n",
    "{'loss': 1.9823, 'grad_norm': 8.076107025146484, 'learning_rate': 3.5365853658536588e-06, 'epoch': 0.83}\n",
    "{'loss': 1.9311, 'grad_norm': 8.174904823303223, 'learning_rate': 3.414634146341464e-06, 'epoch': 0.84}\n",
    "{'loss': 1.861, 'grad_norm': 8.315717697143555, 'learning_rate': 3.292682926829269e-06, 'epoch': 0.84}\n",
    "{'loss': 1.9824, 'grad_norm': 8.190650939941406, 'learning_rate': 3.1707317073170736e-06, 'epoch': 0.85}\n",
    "{'loss': 1.8464, 'grad_norm': 7.917259693145752, 'learning_rate': 3.0487804878048782e-06, 'epoch': 0.86}\n",
    "{'loss': 1.9039, 'grad_norm': 8.007320404052734, 'learning_rate': 2.926829268292683e-06, 'epoch': 0.86}\n",
    "{'loss': 1.8056, 'grad_norm': 8.437159538269043, 'learning_rate': 2.8048780487804884e-06, 'epoch': 0.87}\n",
    "{'loss': 1.9748, 'grad_norm': 8.192963600158691, 'learning_rate': 2.682926829268293e-06, 'epoch': 0.87}\n",
    "{'loss': 1.9233, 'grad_norm': 8.65202808380127, 'learning_rate': 2.5609756097560977e-06, 'epoch': 0.88}\n",
    "{'loss': 1.9247, 'grad_norm': 8.896955490112305, 'learning_rate': 2.4390243902439027e-06, 'epoch': 0.88}\n",
    "{'loss': 2.1176, 'grad_norm': 9.665241241455078, 'learning_rate': 2.317073170731708e-06, 'epoch': 0.89}\n",
    "{'loss': 1.7775, 'grad_norm': 8.175864219665527, 'learning_rate': 2.1951219512195125e-06, 'epoch': 0.9}\n",
    "{'loss': 1.8372, 'grad_norm': 8.083745002746582, 'learning_rate': 2.073170731707317e-06, 'epoch': 0.9}\n",
    "{'loss': 2.1474, 'grad_norm': 8.822936058044434, 'learning_rate': 1.951219512195122e-06, 'epoch': 0.91}\n",
    "{'loss': 1.8599, 'grad_norm': 8.57568359375, 'learning_rate': 1.8292682926829268e-06, 'epoch': 0.91}\n",
    "{'loss': 1.9007, 'grad_norm': 8.18412971496582, 'learning_rate': 1.707317073170732e-06, 'epoch': 0.92}\n",
    "{'loss': 2.0991, 'grad_norm': 8.250720024108887, 'learning_rate': 1.5853658536585368e-06, 'epoch': 0.93}\n",
    "{'loss': 1.9722, 'grad_norm': 8.910171508789062, 'learning_rate': 1.4634146341463414e-06, 'epoch': 0.93}\n",
    "{'loss': 2.0934, 'grad_norm': 8.386656761169434, 'learning_rate': 1.3414634146341465e-06, 'epoch': 0.94}\n",
    "{'loss': 2.0222, 'grad_norm': 8.357674598693848, 'learning_rate': 1.2195121951219514e-06, 'epoch': 0.94}\n",
    "{'loss': 1.9332, 'grad_norm': 8.05184268951416, 'learning_rate': 1.0975609756097562e-06, 'epoch': 0.95}\n",
    "{'loss': 1.912, 'grad_norm': 8.39431381225586, 'learning_rate': 9.75609756097561e-07, 'epoch': 0.96}\n",
    "{'loss': 1.9169, 'grad_norm': 7.906954765319824, 'learning_rate': 8.53658536585366e-07, 'epoch': 0.96}\n",
    "{'loss': 2.034, 'grad_norm': 8.457703590393066, 'learning_rate': 7.317073170731707e-07, 'epoch': 0.97}\n",
    "{'loss': 1.9638, 'grad_norm': 7.690509796142578, 'learning_rate': 6.097560975609757e-07, 'epoch': 0.97}\n",
    "{'loss': 1.7406, 'grad_norm': 8.174431800842285, 'learning_rate': 4.878048780487805e-07, 'epoch': 0.98}\n",
    "{'loss': 2.0476, 'grad_norm': 8.016519546508789, 'learning_rate': 3.6585365853658536e-07, 'epoch': 0.99}\n",
    "{'loss': 1.7636, 'grad_norm': 8.189172744750977, 'learning_rate': 2.439024390243903e-07, 'epoch': 0.99}\n",
    "{'loss': 1.8969, 'grad_norm': 7.952329158782959, 'learning_rate': 1.2195121951219514e-07, 'epoch': 1.0}\n",
    "100%|█████████████████████████████████████████| 169/169 [05:00<00:00,  1.74s/it]swanlab: Step 169 on key train/epoch already exists, ignored.\n",
    "swanlab: Step 169 on key train/global_step already exists, ignored.\n",
    "{'train_runtime': 306.116, 'train_samples_per_second': 8.85, 'train_steps_per_second': 0.552, 'train_loss': 2.031245918668939, 'epoch': 1.0}\n",
    "100%|█████████████████████████████████████████| 169/169 [05:04<00:00,  1.80s/it]\n",
    "Renamed ./lora/checkpoint-169 to ./12outputnew4CC\n",
    "swanlab: Experiment ./lora has completed\n",
    "swanlab: 🏠 View project at https://swanlab.cn/@queziaa/llm\n",
    "swanlab: 🚀 View run at https://swanlab.cn/@queziaa/llm/runs/1wivcymfeu0dmaaqe6lgl\n",
    "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.                           \n",
    "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
    "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
    "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.51.3.\n",
    "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 22.159 GB. Platform: Linux.\n",
    "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
    "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
    " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
    "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
    "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.33it/s]\n",
    "Unsloth 2025.3.19 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n",
    "Unsloth: Tokenizing [\"prompt\"] (num_proc=2): 100%|█| 3517/3517 [00:02<00:00, 122\n",
    "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
    "   \\\\   /|    Num examples = 3,517 | Num Epochs = 1 | Total steps = 220\n",
    "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
    "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
    " \"-____-\"     Trainable parameters = 322,961,408/7,000,000,000 (4.61% trained)\n",
    "swanlab: Tracking run with swanlab version 0.5.5                                   \n",
    "swanlab: Run data will be saved locally in /llm/swanlog/run-20250420_160531-0e8cd89d\n",
    "swanlab: 👋 Hi queziaa, welcome to swanlab!\n",
    "swanlab: Syncing run ./lora to the cloud\n",
    "swanlab: 🏠 View project at https://swanlab.cn/@queziaa/llm\n",
    "swanlab: 🚀 View run at https://swanlab.cn/@queziaa/llm/runs/ain72skbxj1yluyfrmt13\n",
    "  0%|                                                   | 0/220 [00:00<?, ?it/s]Unsloth: Will smartly offload gradients to save VRAM!\n",
    "{'loss': 2.6994, 'grad_norm': 14.792101860046387, 'learning_rate': 0.0, 'epoch': 0.0}\n",
    "{'loss': 2.7621, 'grad_norm': 15.153763771057129, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}\n",
    "{'loss': 2.4394, 'grad_norm': 12.308511734008789, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.01}\n",
    "{'loss': 1.9078, 'grad_norm': 10.7522611618042, 'learning_rate': 1.2e-05, 'epoch': 0.02}\n",
    "{'loss': 1.5392, 'grad_norm': 9.033011436462402, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.02}\n",
    "{'loss': 1.2038, 'grad_norm': 7.469221115112305, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
    "{'loss': 1.1191, 'grad_norm': 5.567471981048584, 'learning_rate': 1.990697674418605e-05, 'epoch': 0.03}\n",
    "{'loss': 1.0925, 'grad_norm': 4.824064254760742, 'learning_rate': 1.9813953488372094e-05, 'epoch': 0.04}\n",
    "{'loss': 1.1535, 'grad_norm': 4.709211826324463, 'learning_rate': 1.9720930232558142e-05, 'epoch': 0.04}\n",
    "{'loss': 1.0275, 'grad_norm': 4.988321304321289, 'learning_rate': 1.9627906976744187e-05, 'epoch': 0.05}\n",
    "{'loss': 1.0809, 'grad_norm': 4.635021209716797, 'learning_rate': 1.9534883720930235e-05, 'epoch': 0.05}\n",
    "{'loss': 0.9179, 'grad_norm': 4.138062000274658, 'learning_rate': 1.944186046511628e-05, 'epoch': 0.05}\n",
    "{'loss': 0.8638, 'grad_norm': 3.7528560161590576, 'learning_rate': 1.9348837209302327e-05, 'epoch': 0.06}\n",
    "{'loss': 0.9976, 'grad_norm': 3.703831195831299, 'learning_rate': 1.9255813953488375e-05, 'epoch': 0.06}\n",
    "{'loss': 0.9098, 'grad_norm': 3.77323055267334, 'learning_rate': 1.916279069767442e-05, 'epoch': 0.07}\n",
    "{'loss': 0.9878, 'grad_norm': 4.1519317626953125, 'learning_rate': 1.9069767441860468e-05, 'epoch': 0.07}\n",
    "{'loss': 1.0076, 'grad_norm': 4.115588665008545, 'learning_rate': 1.8976744186046516e-05, 'epoch': 0.08}\n",
    "{'loss': 0.9978, 'grad_norm': 3.875869035720825, 'learning_rate': 1.888372093023256e-05, 'epoch': 0.08}\n",
    "{'loss': 0.9102, 'grad_norm': 3.8194985389709473, 'learning_rate': 1.8790697674418605e-05, 'epoch': 0.09}\n",
    "{'loss': 0.9649, 'grad_norm': 3.8766732215881348, 'learning_rate': 1.8697674418604653e-05, 'epoch': 0.09}\n",
    "{'loss': 0.9479, 'grad_norm': 3.6500301361083984, 'learning_rate': 1.86046511627907e-05, 'epoch': 0.1}\n",
    "{'loss': 0.9304, 'grad_norm': 3.6648428440093994, 'learning_rate': 1.8511627906976745e-05, 'epoch': 0.1}\n",
    "{'loss': 0.9805, 'grad_norm': 3.8764572143554688, 'learning_rate': 1.8418604651162793e-05, 'epoch': 0.1}\n",
    "{'loss': 0.8887, 'grad_norm': 3.8875770568847656, 'learning_rate': 1.8325581395348838e-05, 'epoch': 0.11}\n",
    "{'loss': 1.0095, 'grad_norm': 3.838005542755127, 'learning_rate': 1.8232558139534886e-05, 'epoch': 0.11}\n",
    "{'loss': 0.9562, 'grad_norm': 3.788022756576538, 'learning_rate': 1.813953488372093e-05, 'epoch': 0.12}\n",
    "{'loss': 0.9129, 'grad_norm': 3.763711929321289, 'learning_rate': 1.8046511627906978e-05, 'epoch': 0.12}\n",
    "{'loss': 0.883, 'grad_norm': 3.626856565475464, 'learning_rate': 1.7953488372093023e-05, 'epoch': 0.13}\n",
    "{'loss': 1.04, 'grad_norm': 3.7802960872650146, 'learning_rate': 1.786046511627907e-05, 'epoch': 0.13}\n",
    "{'loss': 0.9135, 'grad_norm': 3.4621663093566895, 'learning_rate': 1.776744186046512e-05, 'epoch': 0.14}\n",
    "{'loss': 0.9991, 'grad_norm': 3.8744125366210938, 'learning_rate': 1.7674418604651163e-05, 'epoch': 0.14}\n",
    "{'loss': 0.9786, 'grad_norm': 3.798088312149048, 'learning_rate': 1.758139534883721e-05, 'epoch': 0.15}\n",
    "{'loss': 0.9828, 'grad_norm': 3.625710964202881, 'learning_rate': 1.748837209302326e-05, 'epoch': 0.15}\n",
    "{'loss': 0.946, 'grad_norm': 3.586041212081909, 'learning_rate': 1.7395348837209304e-05, 'epoch': 0.15}\n",
    "{'loss': 0.9623, 'grad_norm': 3.6850123405456543, 'learning_rate': 1.7302325581395348e-05, 'epoch': 0.16}\n",
    "{'loss': 0.975, 'grad_norm': 3.588116407394409, 'learning_rate': 1.7209302325581396e-05, 'epoch': 0.16}\n",
    "{'loss': 0.9742, 'grad_norm': 3.6751842498779297, 'learning_rate': 1.7116279069767444e-05, 'epoch': 0.17}\n",
    "{'loss': 0.9243, 'grad_norm': 3.4636664390563965, 'learning_rate': 1.702325581395349e-05, 'epoch': 0.17}\n",
    "{'loss': 0.8745, 'grad_norm': 3.427335739135742, 'learning_rate': 1.6930232558139537e-05, 'epoch': 0.18}\n",
    "{'loss': 0.8847, 'grad_norm': 3.4798343181610107, 'learning_rate': 1.6837209302325585e-05, 'epoch': 0.18}\n",
    "{'loss': 0.9217, 'grad_norm': 3.71699595451355, 'learning_rate': 1.674418604651163e-05, 'epoch': 0.19}\n",
    "{'loss': 0.9737, 'grad_norm': 3.6442317962646484, 'learning_rate': 1.6651162790697674e-05, 'epoch': 0.19}\n",
    "{'loss': 0.9362, 'grad_norm': 3.682427167892456, 'learning_rate': 1.6558139534883722e-05, 'epoch': 0.2}\n",
    "{'loss': 0.968, 'grad_norm': 3.809633731842041, 'learning_rate': 1.646511627906977e-05, 'epoch': 0.2}\n",
    "{'loss': 0.9661, 'grad_norm': 3.743021011352539, 'learning_rate': 1.6372093023255814e-05, 'epoch': 0.2}\n",
    "{'loss': 0.9839, 'grad_norm': 3.6889233589172363, 'learning_rate': 1.6279069767441862e-05, 'epoch': 0.21}\n",
    "{'loss': 0.9616, 'grad_norm': 3.7767412662506104, 'learning_rate': 1.618604651162791e-05, 'epoch': 0.21}\n",
    "{'loss': 0.9864, 'grad_norm': 3.8608906269073486, 'learning_rate': 1.6093023255813955e-05, 'epoch': 0.22}\n",
    "{'loss': 1.0424, 'grad_norm': 3.7715060710906982, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.22}\n",
    "{'loss': 0.9409, 'grad_norm': 3.659003257751465, 'learning_rate': 1.5906976744186047e-05, 'epoch': 0.23}\n",
    "{'loss': 0.8919, 'grad_norm': 3.526559591293335, 'learning_rate': 1.5813953488372095e-05, 'epoch': 0.23}\n",
    "{'loss': 0.8999, 'grad_norm': 3.560216188430786, 'learning_rate': 1.572093023255814e-05, 'epoch': 0.24}\n",
    "{'loss': 0.9016, 'grad_norm': 3.60032320022583, 'learning_rate': 1.5627906976744188e-05, 'epoch': 0.24}\n",
    "{'loss': 0.9359, 'grad_norm': 3.5703184604644775, 'learning_rate': 1.5534883720930232e-05, 'epoch': 0.25}\n",
    "{'loss': 0.8919, 'grad_norm': 3.4578781127929688, 'learning_rate': 1.544186046511628e-05, 'epoch': 0.25}\n",
    "{'loss': 1.0351, 'grad_norm': 3.9016566276550293, 'learning_rate': 1.5348837209302328e-05, 'epoch': 0.25}\n",
    "{'loss': 1.0197, 'grad_norm': 3.8756213188171387, 'learning_rate': 1.5255813953488374e-05, 'epoch': 0.26}\n",
    "{'loss': 0.9189, 'grad_norm': 3.7930235862731934, 'learning_rate': 1.5162790697674419e-05, 'epoch': 0.26}\n",
    "{'loss': 1.0031, 'grad_norm': 3.758430242538452, 'learning_rate': 1.5069767441860465e-05, 'epoch': 0.27}\n",
    "{'loss': 0.9119, 'grad_norm': 3.797461986541748, 'learning_rate': 1.4976744186046512e-05, 'epoch': 0.27}\n",
    "{'loss': 0.9211, 'grad_norm': 3.570493459701538, 'learning_rate': 1.488372093023256e-05, 'epoch': 0.28}\n",
    "{'loss': 0.9334, 'grad_norm': 3.4791712760925293, 'learning_rate': 1.4790697674418606e-05, 'epoch': 0.28}\n",
    "{'loss': 0.9002, 'grad_norm': 3.66503643989563, 'learning_rate': 1.4697674418604652e-05, 'epoch': 0.29}\n",
    "{'loss': 1.0857, 'grad_norm': 3.862865686416626, 'learning_rate': 1.46046511627907e-05, 'epoch': 0.29}\n",
    "{'loss': 0.9447, 'grad_norm': 3.790015459060669, 'learning_rate': 1.4511627906976746e-05, 'epoch': 0.3}\n",
    "{'loss': 0.8518, 'grad_norm': 3.511631488800049, 'learning_rate': 1.441860465116279e-05, 'epoch': 0.3}\n",
    "{'loss': 0.8868, 'grad_norm': 3.585554838180542, 'learning_rate': 1.4325581395348837e-05, 'epoch': 0.3}\n",
    "{'loss': 1.0264, 'grad_norm': 3.950470209121704, 'learning_rate': 1.4232558139534885e-05, 'epoch': 0.31}\n",
    "{'loss': 0.9262, 'grad_norm': 3.8641610145568848, 'learning_rate': 1.4139534883720931e-05, 'epoch': 0.31}\n",
    "{'loss': 0.9142, 'grad_norm': 3.7844667434692383, 'learning_rate': 1.4046511627906978e-05, 'epoch': 0.32}\n",
    "{'loss': 0.9674, 'grad_norm': 3.7466461658477783, 'learning_rate': 1.3953488372093025e-05, 'epoch': 0.32}\n",
    "{'loss': 1.077, 'grad_norm': 4.025245189666748, 'learning_rate': 1.3860465116279072e-05, 'epoch': 0.33}\n",
    "{'loss': 0.9462, 'grad_norm': 4.189960956573486, 'learning_rate': 1.3767441860465118e-05, 'epoch': 0.33}\n",
    "{'loss': 1.0044, 'grad_norm': 3.9447343349456787, 'learning_rate': 1.3674418604651163e-05, 'epoch': 0.34}\n",
    "{'loss': 0.9131, 'grad_norm': 3.992607831954956, 'learning_rate': 1.358139534883721e-05, 'epoch': 0.34}\n",
    "{'loss': 0.9419, 'grad_norm': 3.688793659210205, 'learning_rate': 1.3488372093023257e-05, 'epoch': 0.35}\n",
    "{'loss': 0.7869, 'grad_norm': 3.4298806190490723, 'learning_rate': 1.3395348837209303e-05, 'epoch': 0.35}\n",
    "{'loss': 0.9312, 'grad_norm': 3.6854023933410645, 'learning_rate': 1.330232558139535e-05, 'epoch': 0.35}\n",
    "{'loss': 0.9946, 'grad_norm': 3.799675703048706, 'learning_rate': 1.3209302325581397e-05, 'epoch': 0.36}\n",
    "{'loss': 0.9723, 'grad_norm': 4.070898532867432, 'learning_rate': 1.3116279069767443e-05, 'epoch': 0.36}\n",
    "{'loss': 0.9365, 'grad_norm': 3.682262659072876, 'learning_rate': 1.302325581395349e-05, 'epoch': 0.37}\n",
    "{'loss': 1.0141, 'grad_norm': 3.7024893760681152, 'learning_rate': 1.2930232558139534e-05, 'epoch': 0.37}\n",
    "{'loss': 1.0526, 'grad_norm': 3.816230297088623, 'learning_rate': 1.2837209302325582e-05, 'epoch': 0.38}\n",
    "{'loss': 1.0467, 'grad_norm': 3.760532855987549, 'learning_rate': 1.2744186046511629e-05, 'epoch': 0.38}\n",
    "{'loss': 0.9545, 'grad_norm': 3.8396410942077637, 'learning_rate': 1.2651162790697675e-05, 'epoch': 0.39}\n",
    "{'loss': 0.8836, 'grad_norm': 3.507686138153076, 'learning_rate': 1.2558139534883723e-05, 'epoch': 0.39}\n",
    "{'loss': 0.8812, 'grad_norm': 3.62270450592041, 'learning_rate': 1.2465116279069769e-05, 'epoch': 0.4}\n",
    "{'loss': 0.896, 'grad_norm': 3.7333478927612305, 'learning_rate': 1.2372093023255815e-05, 'epoch': 0.4}\n",
    "{'loss': 0.9164, 'grad_norm': 3.540180206298828, 'learning_rate': 1.2279069767441863e-05, 'epoch': 0.4}\n",
    "{'loss': 0.9563, 'grad_norm': 3.685636043548584, 'learning_rate': 1.2186046511627908e-05, 'epoch': 0.41}\n",
    "{'loss': 0.9856, 'grad_norm': 3.679096221923828, 'learning_rate': 1.2093023255813954e-05, 'epoch': 0.41}\n",
    "{'loss': 0.9203, 'grad_norm': 3.5797200202941895, 'learning_rate': 1.2e-05, 'epoch': 0.42}\n",
    "{'loss': 0.9301, 'grad_norm': 3.7964632511138916, 'learning_rate': 1.1906976744186047e-05, 'epoch': 0.42}\n",
    "{'loss': 0.9997, 'grad_norm': 3.688102960586548, 'learning_rate': 1.1813953488372095e-05, 'epoch': 0.43}\n",
    "{'loss': 0.9142, 'grad_norm': 3.6468000411987305, 'learning_rate': 1.172093023255814e-05, 'epoch': 0.43}\n",
    "{'loss': 0.8812, 'grad_norm': 3.6769778728485107, 'learning_rate': 1.1627906976744187e-05, 'epoch': 0.44}\n",
    "{'loss': 0.859, 'grad_norm': 3.591041088104248, 'learning_rate': 1.1534883720930235e-05, 'epoch': 0.44}\n",
    "{'loss': 1.0511, 'grad_norm': 3.801203489303589, 'learning_rate': 1.144186046511628e-05, 'epoch': 0.45}\n",
    "{'loss': 0.9725, 'grad_norm': 3.6997461318969727, 'learning_rate': 1.1348837209302326e-05, 'epoch': 0.45}\n",
    "{'loss': 0.943, 'grad_norm': 3.6394479274749756, 'learning_rate': 1.1255813953488372e-05, 'epoch': 0.45}\n",
    "{'loss': 0.9599, 'grad_norm': 3.7265515327453613, 'learning_rate': 1.116279069767442e-05, 'epoch': 0.46}\n",
    "{'loss': 0.9964, 'grad_norm': 3.6638286113739014, 'learning_rate': 1.1069767441860466e-05, 'epoch': 0.46}\n",
    "{'loss': 0.9268, 'grad_norm': 3.692939281463623, 'learning_rate': 1.0976744186046513e-05, 'epoch': 0.47}\n",
    "{'loss': 0.965, 'grad_norm': 3.7244248390197754, 'learning_rate': 1.088372093023256e-05, 'epoch': 0.47}\n",
    "{'loss': 0.8477, 'grad_norm': 3.6822962760925293, 'learning_rate': 1.0790697674418607e-05, 'epoch': 0.48}\n",
    "{'loss': 0.9029, 'grad_norm': 3.51887583732605, 'learning_rate': 1.0697674418604651e-05, 'epoch': 0.48}\n",
    "{'loss': 0.9031, 'grad_norm': 3.489266872406006, 'learning_rate': 1.0604651162790698e-05, 'epoch': 0.49}\n",
    "{'loss': 0.9587, 'grad_norm': 3.6306049823760986, 'learning_rate': 1.0511627906976744e-05, 'epoch': 0.49}\n",
    "{'loss': 0.8895, 'grad_norm': 3.601747989654541, 'learning_rate': 1.0418604651162792e-05, 'epoch': 0.5}\n",
    "{'loss': 0.9439, 'grad_norm': 3.8548078536987305, 'learning_rate': 1.0325581395348838e-05, 'epoch': 0.5}\n",
    "{'loss': 1.0648, 'grad_norm': 3.8911237716674805, 'learning_rate': 1.0232558139534884e-05, 'epoch': 0.5}\n",
    "{'loss': 1.015, 'grad_norm': 3.7912914752960205, 'learning_rate': 1.0139534883720932e-05, 'epoch': 0.51}\n",
    "{'loss': 0.9225, 'grad_norm': 3.803513288497925, 'learning_rate': 1.0046511627906979e-05, 'epoch': 0.51}\n",
    "{'loss': 0.9134, 'grad_norm': 3.583766460418701, 'learning_rate': 9.953488372093025e-06, 'epoch': 0.52}\n",
    "{'loss': 0.8964, 'grad_norm': 3.6951162815093994, 'learning_rate': 9.860465116279071e-06, 'epoch': 0.52}\n",
    "{'loss': 1.0059, 'grad_norm': 3.8399770259857178, 'learning_rate': 9.767441860465117e-06, 'epoch': 0.53}\n",
    "{'loss': 0.9605, 'grad_norm': 3.6463491916656494, 'learning_rate': 9.674418604651164e-06, 'epoch': 0.53}\n",
    "{'loss': 0.9372, 'grad_norm': 3.575432538986206, 'learning_rate': 9.58139534883721e-06, 'epoch': 0.54}\n",
    "{'loss': 0.9944, 'grad_norm': 3.7000861167907715, 'learning_rate': 9.488372093023258e-06, 'epoch': 0.54}\n",
    "{'loss': 0.9914, 'grad_norm': 3.764307737350464, 'learning_rate': 9.395348837209302e-06, 'epoch': 0.55}\n",
    "{'loss': 0.9469, 'grad_norm': 3.6919844150543213, 'learning_rate': 9.30232558139535e-06, 'epoch': 0.55}\n",
    "{'loss': 0.9659, 'grad_norm': 3.7498366832733154, 'learning_rate': 9.209302325581397e-06, 'epoch': 0.55}\n",
    "{'loss': 1.0236, 'grad_norm': 4.0491414070129395, 'learning_rate': 9.116279069767443e-06, 'epoch': 0.56}\n",
    "{'loss': 1.0675, 'grad_norm': 3.8588521480560303, 'learning_rate': 9.023255813953489e-06, 'epoch': 0.56}\n",
    "{'loss': 0.9477, 'grad_norm': 3.503147840499878, 'learning_rate': 8.930232558139535e-06, 'epoch': 0.57}\n",
    "{'loss': 0.8484, 'grad_norm': 3.535078763961792, 'learning_rate': 8.837209302325582e-06, 'epoch': 0.57}\n",
    "{'loss': 0.9817, 'grad_norm': 3.759416341781616, 'learning_rate': 8.74418604651163e-06, 'epoch': 0.58}\n",
    "{'loss': 0.8985, 'grad_norm': 3.596165895462036, 'learning_rate': 8.651162790697674e-06, 'epoch': 0.58}\n",
    "{'loss': 0.878, 'grad_norm': 3.688455104827881, 'learning_rate': 8.558139534883722e-06, 'epoch': 0.59}\n",
    "{'loss': 0.9298, 'grad_norm': 3.7050962448120117, 'learning_rate': 8.465116279069768e-06, 'epoch': 0.59}\n",
    "{'loss': 0.961, 'grad_norm': 3.8254857063293457, 'learning_rate': 8.372093023255815e-06, 'epoch': 0.6}\n",
    "{'loss': 1.0044, 'grad_norm': 3.887362003326416, 'learning_rate': 8.279069767441861e-06, 'epoch': 0.6}\n",
    "{'loss': 0.9399, 'grad_norm': 3.752077341079712, 'learning_rate': 8.186046511627907e-06, 'epoch': 0.6}\n",
    "{'loss': 0.9241, 'grad_norm': 3.600963592529297, 'learning_rate': 8.093023255813955e-06, 'epoch': 0.61}\n",
    "{'loss': 0.9467, 'grad_norm': 3.836280345916748, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.61}\n",
    "{'loss': 0.8301, 'grad_norm': 3.5932273864746094, 'learning_rate': 7.906976744186048e-06, 'epoch': 0.62}\n",
    "{'loss': 0.8864, 'grad_norm': 3.5942935943603516, 'learning_rate': 7.813953488372094e-06, 'epoch': 0.62}\n",
    "{'loss': 0.8797, 'grad_norm': 3.5825917720794678, 'learning_rate': 7.72093023255814e-06, 'epoch': 0.63}\n",
    "{'loss': 0.8765, 'grad_norm': 3.706528425216675, 'learning_rate': 7.627906976744187e-06, 'epoch': 0.63}\n",
    "{'loss': 0.8714, 'grad_norm': 3.670680284500122, 'learning_rate': 7.534883720930233e-06, 'epoch': 0.64}\n",
    "{'loss': 0.9215, 'grad_norm': 3.6857638359069824, 'learning_rate': 7.44186046511628e-06, 'epoch': 0.64}\n",
    "{'loss': 0.9316, 'grad_norm': 3.690274477005005, 'learning_rate': 7.348837209302326e-06, 'epoch': 0.65}\n",
    "{'loss': 0.9276, 'grad_norm': 3.6917777061462402, 'learning_rate': 7.255813953488373e-06, 'epoch': 0.65}\n",
    "{'loss': 0.9221, 'grad_norm': 3.782609462738037, 'learning_rate': 7.1627906976744185e-06, 'epoch': 0.65}\n",
    "{'loss': 0.9012, 'grad_norm': 3.7402946949005127, 'learning_rate': 7.069767441860466e-06, 'epoch': 0.66}\n",
    "{'loss': 1.021, 'grad_norm': 3.826554536819458, 'learning_rate': 6.976744186046513e-06, 'epoch': 0.66}\n",
    "{'loss': 0.8643, 'grad_norm': 3.5846664905548096, 'learning_rate': 6.883720930232559e-06, 'epoch': 0.67}\n",
    "{'loss': 0.9591, 'grad_norm': 3.798341751098633, 'learning_rate': 6.790697674418605e-06, 'epoch': 0.67}\n",
    "{'loss': 0.8588, 'grad_norm': 3.5989973545074463, 'learning_rate': 6.6976744186046515e-06, 'epoch': 0.68}\n",
    "{'loss': 0.915, 'grad_norm': 3.511711597442627, 'learning_rate': 6.604651162790699e-06, 'epoch': 0.68}\n",
    "{'loss': 1.033, 'grad_norm': 3.989450693130493, 'learning_rate': 6.511627906976745e-06, 'epoch': 0.69}\n",
    "{'loss': 0.9129, 'grad_norm': 3.690189838409424, 'learning_rate': 6.418604651162791e-06, 'epoch': 0.69}\n",
    "{'loss': 1.0883, 'grad_norm': 3.884878396987915, 'learning_rate': 6.325581395348837e-06, 'epoch': 0.7}\n",
    "{'loss': 0.9503, 'grad_norm': 3.721052885055542, 'learning_rate': 6.2325581395348845e-06, 'epoch': 0.7}\n",
    "{'loss': 0.9841, 'grad_norm': 3.78412127494812, 'learning_rate': 6.139534883720932e-06, 'epoch': 0.7}\n",
    "{'loss': 1.1151, 'grad_norm': 3.9428181648254395, 'learning_rate': 6.046511627906977e-06, 'epoch': 0.71}\n",
    "{'loss': 0.8922, 'grad_norm': 3.5945091247558594, 'learning_rate': 5.953488372093023e-06, 'epoch': 0.71}\n",
    "{'loss': 1.0883, 'grad_norm': 3.845427989959717, 'learning_rate': 5.86046511627907e-06, 'epoch': 0.72}\n",
    "{'loss': 0.9564, 'grad_norm': 3.7623884677886963, 'learning_rate': 5.7674418604651175e-06, 'epoch': 0.72}\n",
    "{'loss': 1.0158, 'grad_norm': 3.7944979667663574, 'learning_rate': 5.674418604651163e-06, 'epoch': 0.73}\n",
    "{'loss': 1.0355, 'grad_norm': 3.8116133213043213, 'learning_rate': 5.58139534883721e-06, 'epoch': 0.73}\n",
    "{'loss': 0.9688, 'grad_norm': 3.667226552963257, 'learning_rate': 5.488372093023256e-06, 'epoch': 0.74}\n",
    "{'loss': 0.8951, 'grad_norm': 3.5459914207458496, 'learning_rate': 5.395348837209303e-06, 'epoch': 0.74}\n",
    "{'loss': 0.9853, 'grad_norm': 3.5500588417053223, 'learning_rate': 5.302325581395349e-06, 'epoch': 0.75}\n",
    "{'loss': 0.9723, 'grad_norm': 3.643758535385132, 'learning_rate': 5.209302325581396e-06, 'epoch': 0.75}\n",
    "{'loss': 1.0342, 'grad_norm': 3.6228997707366943, 'learning_rate': 5.116279069767442e-06, 'epoch': 0.75}\n",
    "{'loss': 0.991, 'grad_norm': 3.663754463195801, 'learning_rate': 5.023255813953489e-06, 'epoch': 0.76}\n",
    "{'loss': 0.9799, 'grad_norm': 3.6790809631347656, 'learning_rate': 4.9302325581395355e-06, 'epoch': 0.76}\n",
    "{'loss': 0.9613, 'grad_norm': 3.522874116897583, 'learning_rate': 4.837209302325582e-06, 'epoch': 0.77}\n",
    "{'loss': 0.9289, 'grad_norm': 3.616431951522827, 'learning_rate': 4.744186046511629e-06, 'epoch': 0.77}\n",
    "{'loss': 0.9802, 'grad_norm': 3.7106354236602783, 'learning_rate': 4.651162790697675e-06, 'epoch': 0.78}\n",
    "{'loss': 0.9355, 'grad_norm': 3.588930606842041, 'learning_rate': 4.558139534883721e-06, 'epoch': 0.78}\n",
    "{'loss': 0.9215, 'grad_norm': 3.5238587856292725, 'learning_rate': 4.465116279069768e-06, 'epoch': 0.79}\n",
    "{'loss': 0.9836, 'grad_norm': 3.834394931793213, 'learning_rate': 4.372093023255815e-06, 'epoch': 0.79}\n",
    "{'loss': 1.1207, 'grad_norm': 3.744262456893921, 'learning_rate': 4.279069767441861e-06, 'epoch': 0.8}\n",
    "{'loss': 1.0402, 'grad_norm': 3.8620290756225586, 'learning_rate': 4.186046511627907e-06, 'epoch': 0.8}\n",
    "{'loss': 0.8491, 'grad_norm': 3.5921881198883057, 'learning_rate': 4.0930232558139536e-06, 'epoch': 0.8}\n",
    "{'loss': 0.9128, 'grad_norm': 3.4866974353790283, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.81}\n",
    "{'loss': 0.8932, 'grad_norm': 3.60564923286438, 'learning_rate': 3.906976744186047e-06, 'epoch': 0.81}\n",
    "{'loss': 0.9858, 'grad_norm': 3.6569719314575195, 'learning_rate': 3.8139534883720936e-06, 'epoch': 0.82}\n",
    "{'loss': 0.9829, 'grad_norm': 3.7439723014831543, 'learning_rate': 3.72093023255814e-06, 'epoch': 0.82}\n",
    "{'loss': 0.978, 'grad_norm': 3.8479039669036865, 'learning_rate': 3.6279069767441866e-06, 'epoch': 0.83}\n",
    "{'loss': 0.976, 'grad_norm': 3.85090970993042, 'learning_rate': 3.534883720930233e-06, 'epoch': 0.83}\n",
    "{'loss': 1.0906, 'grad_norm': 3.9640653133392334, 'learning_rate': 3.4418604651162795e-06, 'epoch': 0.84}\n",
    "{'loss': 0.9828, 'grad_norm': 3.5556106567382812, 'learning_rate': 3.3488372093023258e-06, 'epoch': 0.84}\n",
    "{'loss': 0.9381, 'grad_norm': 3.6262052059173584, 'learning_rate': 3.2558139534883724e-06, 'epoch': 0.85}\n",
    "{'loss': 0.9879, 'grad_norm': 3.8259029388427734, 'learning_rate': 3.1627906976744187e-06, 'epoch': 0.85}\n",
    "{'loss': 1.0024, 'grad_norm': 3.8008944988250732, 'learning_rate': 3.069767441860466e-06, 'epoch': 0.85}\n",
    "{'loss': 0.9931, 'grad_norm': 3.9472813606262207, 'learning_rate': 2.9767441860465116e-06, 'epoch': 0.86}\n",
    "{'loss': 1.0159, 'grad_norm': 3.7458419799804688, 'learning_rate': 2.8837209302325587e-06, 'epoch': 0.86}\n",
    "{'loss': 0.9078, 'grad_norm': 3.6026339530944824, 'learning_rate': 2.790697674418605e-06, 'epoch': 0.87}\n",
    "{'loss': 0.9123, 'grad_norm': 3.694927453994751, 'learning_rate': 2.6976744186046517e-06, 'epoch': 0.87}\n",
    "{'loss': 1.0231, 'grad_norm': 3.801767110824585, 'learning_rate': 2.604651162790698e-06, 'epoch': 0.88}\n",
    "{'loss': 0.9947, 'grad_norm': 3.7251219749450684, 'learning_rate': 2.5116279069767446e-06, 'epoch': 0.88}\n",
    "{'loss': 1.1158, 'grad_norm': 3.870746612548828, 'learning_rate': 2.418604651162791e-06, 'epoch': 0.89}\n",
    "{'loss': 1.1403, 'grad_norm': 4.017486572265625, 'learning_rate': 2.3255813953488376e-06, 'epoch': 0.89}\n",
    "{'loss': 1.0774, 'grad_norm': 3.8498246669769287, 'learning_rate': 2.232558139534884e-06, 'epoch': 0.9}\n",
    "{'loss': 0.9154, 'grad_norm': 3.650987386703491, 'learning_rate': 2.1395348837209305e-06, 'epoch': 0.9}\n",
    "{'loss': 0.9085, 'grad_norm': 3.6531577110290527, 'learning_rate': 2.0465116279069768e-06, 'epoch': 0.9}\n",
    "{'loss': 0.9863, 'grad_norm': 3.680453062057495, 'learning_rate': 1.9534883720930235e-06, 'epoch': 0.91}\n",
    "{'loss': 1.0286, 'grad_norm': 3.884459972381592, 'learning_rate': 1.86046511627907e-06, 'epoch': 0.91}\n",
    "{'loss': 1.2319, 'grad_norm': 4.135828971862793, 'learning_rate': 1.7674418604651164e-06, 'epoch': 0.92}\n",
    "{'loss': 1.0296, 'grad_norm': 3.7031562328338623, 'learning_rate': 1.6744186046511629e-06, 'epoch': 0.92}\n",
    "{'loss': 0.9019, 'grad_norm': 3.7730367183685303, 'learning_rate': 1.5813953488372093e-06, 'epoch': 0.93}\n",
    "{'loss': 0.967, 'grad_norm': 3.8172504901885986, 'learning_rate': 1.4883720930232558e-06, 'epoch': 0.93}\n",
    "{'loss': 0.9314, 'grad_norm': 3.6821324825286865, 'learning_rate': 1.3953488372093025e-06, 'epoch': 0.94}\n",
    "{'loss': 1.0935, 'grad_norm': 3.889313220977783, 'learning_rate': 1.302325581395349e-06, 'epoch': 0.94}\n",
    "{'loss': 1.0641, 'grad_norm': 3.7284696102142334, 'learning_rate': 1.2093023255813954e-06, 'epoch': 0.95}\n",
    "{'loss': 0.9538, 'grad_norm': 3.4879822731018066, 'learning_rate': 1.116279069767442e-06, 'epoch': 0.95}\n",
    "{'loss': 0.9856, 'grad_norm': 3.4624581336975098, 'learning_rate': 1.0232558139534884e-06, 'epoch': 0.95}\n",
    "{'loss': 1.0243, 'grad_norm': 3.5357308387756348, 'learning_rate': 9.30232558139535e-07, 'epoch': 0.96}\n",
    "{'loss': 0.9551, 'grad_norm': 3.813525438308716, 'learning_rate': 8.372093023255814e-07, 'epoch': 0.96}\n",
    "{'loss': 0.9921, 'grad_norm': 3.7190983295440674, 'learning_rate': 7.441860465116279e-07, 'epoch': 0.97}\n",
    "{'loss': 1.0756, 'grad_norm': 3.8314387798309326, 'learning_rate': 6.511627906976745e-07, 'epoch': 0.97}\n",
    "{'loss': 1.0632, 'grad_norm': 3.699403762817383, 'learning_rate': 5.58139534883721e-07, 'epoch': 0.98}\n",
    "{'loss': 1.0102, 'grad_norm': 3.657588481903076, 'learning_rate': 4.651162790697675e-07, 'epoch': 0.98}\n",
    "{'loss': 0.9824, 'grad_norm': 3.7354469299316406, 'learning_rate': 3.7209302325581396e-07, 'epoch': 0.99}\n",
    "{'loss': 0.9519, 'grad_norm': 3.582408905029297, 'learning_rate': 2.790697674418605e-07, 'epoch': 0.99}\n",
    "{'loss': 0.9909, 'grad_norm': 3.8126721382141113, 'learning_rate': 1.8604651162790698e-07, 'epoch': 1.0}\n",
    "{'loss': 0.9382, 'grad_norm': 3.9135828018188477, 'learning_rate': 9.302325581395349e-08, 'epoch': 1.0}\n",
    "100%|█████████████████████████████████████████| 220/220 [12:06<00:00,  3.41s/it]swanlab: Step 220 on key train/epoch already exists, ignored.\n",
    "swanlab: Step 220 on key train/global_step already exists, ignored.\n",
    "{'train_runtime': 733.2742, 'train_samples_per_second': 4.796, 'train_steps_per_second': 0.3, 'train_loss': 0.9949806145646355, 'epoch': 1.0}\n",
    "100%|█████████████████████████████████████████| 220/220 [12:11<00:00,  3.33s/it]\n",
    "Renamed ./lora/checkpoint-220 to ./3outputnew3CC\n",
    "swanlab: Experiment ./lora has completed\n",
    "swanlab: 🏠 View project at https://swanlab.cn/@queziaa/llm\n",
    "swanlab: 🚀 View run at https://swanlab.cn/@queziaa/llm/runs/ain72skbxj1yluyfrmt13\n",
    "                                                                                                    \n",
    "                                                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccbd1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python  t_GRPO.py --config GRPO.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
