{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0009e6cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68d1810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n",
      "                                                text  label\n",
      "0  来中国的黑人，大概率和国女谈过恋爱，或者发生过关系，好多不止谈过一个。个人感觉，国内黑人有中...      1\n",
      "1  这个吧里都是**有个女的喷我然后让我教育了自己外地的去新一线然后瞧不起我大连土著说我是18线...      0\n",
      "2  如果你有数据就拿出数据说明是大多数，不要在这里'…吧'。简单说几点，艾滋病保密原则，留学生特...      1\n",
      "3      如果说要反对嘿人留在中国，必须真实点赞率必须超过4.7亿人，也就是总人口的1/3以上！！！      0\n",
      "4               是这样的，这种极端言论感觉调调都差不多，就像是什么地方专门训练出来的一样      0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/queziaa/.local/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "训练中...\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "# 设置随机种子，保证可复现性\n",
    "def set_seed(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 检查是否有GPU可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 创建一个简单的二分类数据集（正面评论和负面评论）\n",
    "# 在实际应用中，您应该替换为您自己的数据集\n",
    "positive_texts = []\n",
    "\n",
    "negative_texts = []\n",
    "\n",
    "\n",
    "import json\n",
    "with open('./DATA/train.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "for i in data:\n",
    "    if '[SEP]' in i['output']:\n",
    "        positive_texts.append(i['content'])\n",
    "    else:\n",
    "        negative_texts.append(i['content'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 构建数据集\n",
    "texts = positive_texts + negative_texts\n",
    "labels = [1] * len(positive_texts) + [0] * len(negative_texts)\n",
    "df = pd.DataFrame({\"text\": texts, \"label\": labels})\n",
    "\n",
    "# 打乱数据\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "print(df.head())\n",
    "\n",
    "# 加载中文BERT的tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "# 对文本进行编码\n",
    "def encode_texts(texts, max_length=64):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for text in texts:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n",
    "\n",
    "# 划分训练集和验证集\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df['text'].values, \n",
    "    df['label'].values, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df['label'].values\n",
    ")\n",
    "\n",
    "# 编码训练集和验证集\n",
    "train_inputs, train_masks = encode_texts(X_train)\n",
    "val_inputs, val_masks = encode_texts(X_val)\n",
    "\n",
    "# 转换为PyTorch张量\n",
    "train_labels = torch.tensor(y_train)\n",
    "val_labels = torch.tensor(y_val)\n",
    "\n",
    "# 创建数据加载器\n",
    "batch_size = 4\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "# 加载预训练的中文BERT模型\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-chinese',\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# 设置优化器\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "# 定义训练轮次\n",
    "epochs = 4\n",
    "\n",
    "# 计算总训练步数\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# 创建学习率调度器\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# 定义训练函数\n",
    "def train_model(model, train_dataloader, val_dataloader, epochs, optimizer, scheduler, device):\n",
    "    # 用于存储训练和评估损失\n",
    "    training_stats = []\n",
    "    \n",
    "    # 测量整个训练时间\n",
    "    total_t0 = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n======== Epoch {epoch+1} / {epochs} ========\")\n",
    "        print('训练中...')\n",
    "        \n",
    "        # 重置每个epoch的总损失\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        # 将模型置于训练模式\n",
    "        model.train()\n",
    "        \n",
    "        # 对dataloader中的每个batch进行训练\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # 每隔40个batch输出一次进度\n",
    "            if step % 100 == 0 and not step == 0:\n",
    "                print(f'  Batch {step} / {len(train_dataloader)}')\n",
    "            \n",
    "            # 解包batch中的数据\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            \n",
    "            # 清除之前的梯度\n",
    "            model.zero_grad()        \n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = model(b_input_ids, \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=b_input_mask, \n",
    "                           labels=b_labels)\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            \n",
    "            # 梯度裁剪，避免梯度爆炸\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            # 更新参数\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 更新学习率\n",
    "            scheduler.step()\n",
    "        \n",
    "        # 计算平均损失\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        \n",
    "        print(f\"\\n  Average training loss: {avg_train_loss:.2f}\")\n",
    "        \n",
    "        print(\"\\n验证中...\")\n",
    "        \n",
    "        # 将模型置于评估模式\n",
    "        model.eval()\n",
    "        \n",
    "        # 追踪变量\n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        all_predictions = []\n",
    "        all_true_labels = []\n",
    "        \n",
    "        # 评估数据\n",
    "        for batch in val_dataloader:\n",
    "            # 解包batch中的数据\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            \n",
    "            # 不计算梯度\n",
    "            with torch.no_grad():\n",
    "                outputs = model(b_input_ids, \n",
    "                               token_type_ids=None, \n",
    "                               attention_mask=b_input_mask,\n",
    "                               labels=b_labels)\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_eval_loss += loss.item()\n",
    "            \n",
    "            # 获取预测结果\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            # 保存预测和真实标签\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_true_labels.extend(b_labels.cpu().numpy())\n",
    "            \n",
    "        # 计算准确率\n",
    "        accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "        f1 = f1_score(all_true_labels, all_predictions)\n",
    "        \n",
    "        # 计算混淆矩阵\n",
    "        cm = confusion_matrix(all_true_labels, all_predictions)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        \n",
    "        print(f\"  混淆矩阵详情:\")\n",
    "        print(f\"  真正例 (TP): {tp}\")\n",
    "        print(f\"  假正例 (FP): {fp}\")\n",
    "        print(f\"  假负例 (FN): {fn}\")\n",
    "        print(f\"  真负例 (TN): {tn}\")\n",
    "        # 计算平均损失\n",
    "        avg_val_loss = total_eval_loss / len(val_dataloader)\n",
    "        \n",
    "        print(f\"  Validation Loss: {avg_val_loss:.2f}\")\n",
    "        print(f\"  Accuracy: {accuracy:.2f}\")\n",
    "        print(f\"  F1 Score: {f1:.2f}\")\n",
    "        \n",
    "        # 保存本轮的统计数据\n",
    "        training_stats.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Validation Loss': avg_val_loss,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1 Score': f1\n",
    "        })\n",
    "    \n",
    "    print(\"\\n训练完成!\")\n",
    "    \n",
    "    print(f\"总训练时间: {(time.time() - total_t0):.2f} 秒\")\n",
    "    \n",
    "    return model, training_stats\n",
    "\n",
    "# 训练模型\n",
    "model, stats = train_model(model, train_dataloader, val_dataloader, epochs, optimizer, scheduler, device)\n",
    "\n",
    "# 可视化训练结果\n",
    "df_stats = pd.DataFrame(stats)\n",
    "print(df_stats)\n",
    "\n",
    "# 绘制训练和验证损失曲线\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(df_stats['Training Loss'], label='Training Loss')\n",
    "plt.plot(df_stats['Validation Loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 绘制混淆矩阵\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for batch in val_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    preds = torch.argmax(logits, dim=1).flatten()\n",
    "    \n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "    true_labels.extend(b_labels.cpu().numpy())\n",
    "\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['负面', '正面'], yticklabels=['负面', '正面'])\n",
    "plt.title('混淆矩阵')\n",
    "plt.ylabel('真实标签')\n",
    "plt.xlabel('预测标签')\n",
    "plt.show()\n",
    "\n",
    "# 保存模型\n",
    "model_path = \"./chinese_bert_classifier\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "print(f\"模型已保存到 {model_path}\")\n",
    "\n",
    "# 进行预测示例\n",
    "def predict_sentiment(text, model, tokenizer):\n",
    "    # 对文本进行编码\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=256,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    \n",
    "    # 将编码后的数据移到指定设备\n",
    "    input_ids = encoded_dict['input_ids'].to(device)\n",
    "    attention_mask = encoded_dict['attention_mask'].to(device)\n",
    "    \n",
    "    # 设置模型为评估模式\n",
    "    model.eval()\n",
    "    \n",
    "    # 进行预测\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_mask)\n",
    "    \n",
    "    # 获取预测结果\n",
    "    logits = outputs.logits\n",
    "    prediction = torch.argmax(logits, dim=1).item()\n",
    "    confidence = torch.nn.functional.softmax(logits, dim=1)[0][prediction].item()\n",
    "    \n",
    "    # 返回预测结果和置信度\n",
    "    sentiment = \"正面\" if prediction == 1 else \"负面\"\n",
    "    return sentiment, confidence\n",
    "\n",
    "# 测试一些新的句子\n",
    "test_sentences = [\n",
    "    \"我觉得这个产品非常好用，推荐购买。\",\n",
    "    \"服务态度极差，以后再也不会光顾了。\",\n",
    "    \"这家餐厅的菜品一般，但是环境不错。\",\n",
    "    \"这部电影剧情老套，演员表演也很一般。\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    sentiment, confidence = predict_sentiment(sentence, model, tokenizer)\n",
    "    print(f\"句子: {sentence}\")\n",
    "    print(f\"情感预测: {sentiment}, 置信度: {confidence:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616966ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
