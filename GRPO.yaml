# use_torch_compile: False
# model_name_or_path: ./c100u
model_name_or_path: pass
# model_name_or_path: ./unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit
# c100 = Q7_SFT12_4_SFT12_3_SFT3_SFT4_SFT13t_SFT13ts_GRPO12C100
model_revision: main
torch_dtype: bfloat16
# lora_r: 128 # Lora 秩数
# lora_alpha: 256 # Lora alpha
# lora_dropout: 0.1 # Lora dropout
bf16: true
tf32: true
output_dir: psss

swanlab: true # 是否开启 Swanlab 
workspace: queziaa
project: llama-fine
experiment_name: GRPO

# max_steps: 450 # 最大训练步长
num_train_epochs: 1 # 训练一轮完整数据集
per_device_train_batch_size: 1
gradient_accumulation_steps: 4
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
learning_rate: 1.0e-5 # 学习率调整为1e-5
lr_scheduler_type: cosine # 学习率衰减方案
warmup_ratio: 0.03 # 学习率预热比率（对于整个步长），好用！
seed: 3407 # 随机种子，方便实验复现

beta: 0.001 # KL 惩罚因子，调整过，参见下文介绍
optim: paged_adamw_32bit 
max_prompt_length: 256 # 输入 prompt 最大长度，本实验基本不会有太大变化
max_new_tokens: 512 # 输出回答长度，包含推理思维链
max_completion_length: 512 # 输出回答长度，包含推理思维链
num_generations: 8
do_sample: true # 是否启用采样
temperature: 0.7 # 采样温度
top_p: 0.9 # 采样 top_p
top_k: 50 # 采样 top_k
max_grad_norm: 0.1
repetition_penalty : 1.2 # 重复惩罚

# use_vllm: true # 启用 vllm 来加速推理
# vllm_gpu_memory_utilization: 0.85
# 8 45  
# 9 COOM
#  False 58

bnb_4bit_quant_storage: uint8
# Logging arguments
logging_strategy: steps
logging_steps: 1
save_strategy: "steps"
save_steps: 10 # 每隔多少步保存一次


    # learning_rate = 5e-6,
    # weight_decay = 0.1,
    # warmup_ratio = 0.1,
    # lr_scheduler_type = "cosine",
    # optim = "adamw_torch_fused",
    # logging_steps = 1,
    # per_device_train_batch_size = 1,
    # gradient_accumulation_steps = 4, # Increase to 4 for smoother training
    # num_generations = 8, # Decrease if out of memory
    # max_prompt_length = max_prompt_length,
    # max_completion_length = max_seq_length - max_prompt_length,
    # # num_train_epochs = 1, # Set to 1 for a full training run
    # max_steps = 1000,
    # save_steps = 250,
    # report_to = "none", # Can use Weights & Biases
    # output_dir = "outputs",
